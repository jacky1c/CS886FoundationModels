{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'iconqa' # 'scienceqa' # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('../input/results/iconqa', ['iconqa_1', 'iconqa_17', 'iconqa_2', 'iconqa_18', 'iconqa_6', 'iconqa_5', 'iconqa_21', 'iconqa_22', 'iconqa_10', 'iconqa_9', 'iconqa_25', 'iconqa_26', 'iconqa_30', 'iconqa_4', 'iconqa_8', 'iconqa_12', 'iconqa_14', 'iconqa_13', 'iconqa_29', 'iconqa_3', 'iconqa_7', 'iconqa_11', 'iconqa_15', 'iconqa_19', 'iconqa_16', 'iconqa_20', 'iconqa_27', 'iconqa_31', 'iconqa_32', 'iconqa_23', 'iconqa_24', 'iconqa_28'], [])\n",
      "('../input/results/iconqa/iconqa_1', ['20240328163', '20240328165', '20240403171', '20240403173_good'], [])\n",
      "('../input/results/iconqa/iconqa_1/20240328163', ['result'], ['log.txt'])\n",
      "('../input/results/iconqa/iconqa_1/20240328163/result', [], [])\n",
      "('../input/results/iconqa/iconqa_1/20240328165', ['result'], ['log.txt'])\n",
      "('../input/results/iconqa/iconqa_1/20240328165/result', [], [])\n",
      "('../input/results/iconqa/iconqa_1/20240403171', ['result'], ['log.txt'])\n",
      "('../input/results/iconqa/iconqa_1/20240403171/result', [], [])\n",
      "('../input/results/iconqa/iconqa_1/20240403173_good', ['result'], ['log.txt', 'iconqa_1.log'])\n",
      "('../input/results/iconqa/iconqa_1/20240403173_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_17', ['20240403173_good'], [])\n",
      "('../input/results/iconqa/iconqa_17/20240403173_good', ['result'], ['log.txt', 'iconqa_17.log'])\n",
      "('../input/results/iconqa/iconqa_17/20240403173_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_2', ['20240403173_good'], [])\n",
      "('../input/results/iconqa/iconqa_2/20240403173_good', ['result'], ['log.txt', 'iconqa_2.log'])\n",
      "('../input/results/iconqa/iconqa_2/20240403173_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_18', ['20240403173_good'], [])\n",
      "('../input/results/iconqa/iconqa_18/20240403173_good', ['result'], ['log.txt', 'iconqa_18.log'])\n",
      "('../input/results/iconqa/iconqa_18/20240403173_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_6', ['20240404012_good'], [])\n",
      "('../input/results/iconqa/iconqa_6/20240404012_good', ['result'], ['log.txt', 'iconqa_6.log'])\n",
      "('../input/results/iconqa/iconqa_6/20240404012_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_5', ['20240404020_good'], [])\n",
      "('../input/results/iconqa/iconqa_5/20240404020_good', ['result'], ['log.txt', 'iconqa_5.log'])\n",
      "('../input/results/iconqa/iconqa_5/20240404020_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_21', ['20240404040_good'], [])\n",
      "('../input/results/iconqa/iconqa_21/20240404040_good', ['result'], ['log.txt', 'iconqa_21.log'])\n",
      "('../input/results/iconqa/iconqa_21/20240404040_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_22', ['20240404040_good'], [])\n",
      "('../input/results/iconqa/iconqa_22/20240404040_good', ['result'], ['log.txt', 'iconqa_22.log'])\n",
      "('../input/results/iconqa/iconqa_22/20240404040_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_10', ['20240404091_good'], [])\n",
      "('../input/results/iconqa/iconqa_10/20240404091_good', ['result'], ['log.txt', 'iconqa_10.log'])\n",
      "('../input/results/iconqa/iconqa_10/20240404091_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_9', ['20240404133_good'], [])\n",
      "('../input/results/iconqa/iconqa_9/20240404133_good', ['result'], ['log.txt', 'iconqa_9.log'])\n",
      "('../input/results/iconqa/iconqa_9/20240404133_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_25', ['20240404142_good'], [])\n",
      "('../input/results/iconqa/iconqa_25/20240404142_good', ['result'], ['log.txt', 'iconqa_25.log'])\n",
      "('../input/results/iconqa/iconqa_25/20240404142_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_26', ['20240404142_good'], [])\n",
      "('../input/results/iconqa/iconqa_26/20240404142_good', ['result'], ['log.txt', 'iconqa_26.log'])\n",
      "('../input/results/iconqa/iconqa_26/20240404142_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_30', ['20240404205_good'], [])\n",
      "('../input/results/iconqa/iconqa_30/20240404205_good', ['result'], ['log.txt', 'iconqa_30.log'])\n",
      "('../input/results/iconqa/iconqa_30/20240404205_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_4', ['20240404211_good'], [])\n",
      "('../input/results/iconqa/iconqa_4/20240404211_good', ['result'], ['log.txt', 'iconqa_4.log'])\n",
      "('../input/results/iconqa/iconqa_4/20240404211_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_8', ['20240404211_good'], [])\n",
      "('../input/results/iconqa/iconqa_8/20240404211_good', ['result'], ['log.txt', 'iconqa_8.log'])\n",
      "('../input/results/iconqa/iconqa_8/20240404211_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_12', ['20240404211_good'], [])\n",
      "('../input/results/iconqa/iconqa_12/20240404211_good', ['result'], ['log.txt', 'iconqa_12.log'])\n",
      "('../input/results/iconqa/iconqa_12/20240404211_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_14', ['20240404221_good'], [])\n",
      "('../input/results/iconqa/iconqa_14/20240404221_good', ['result'], ['log.txt', 'iconqa_14.log'])\n",
      "('../input/results/iconqa/iconqa_14/20240404221_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_13', ['20240404235_good'], [])\n",
      "('../input/results/iconqa/iconqa_13/20240404235_good', ['result'], ['log.txt', 'iconqa_13.log'])\n",
      "('../input/results/iconqa/iconqa_13/20240404235_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_29', ['20240405021_good'], [])\n",
      "('../input/results/iconqa/iconqa_29/20240405021_good', ['result'], ['log.txt', 'iconqa_29.log'])\n",
      "('../input/results/iconqa/iconqa_29/20240405021_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_3', ['20240405174_good'], [])\n",
      "('../input/results/iconqa/iconqa_3/20240405174_good', ['result'], ['log.txt', 'iconqa_3.log'])\n",
      "('../input/results/iconqa/iconqa_3/20240405174_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_7', ['20240406052_good'], [])\n",
      "('../input/results/iconqa/iconqa_7/20240406052_good', ['result'], ['log.txt', 'iconqa_7.log'])\n",
      "('../input/results/iconqa/iconqa_7/20240406052_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_11', ['20240405174_good'], [])\n",
      "('../input/results/iconqa/iconqa_11/20240405174_good', ['result'], ['log.txt', 'iconqa_11.log'])\n",
      "('../input/results/iconqa/iconqa_11/20240405174_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_15', ['20240406005_good'], [])\n",
      "('../input/results/iconqa/iconqa_15/20240406005_good', ['result'], ['log.txt', 'iconqa_15.log'])\n",
      "('../input/results/iconqa/iconqa_15/20240406005_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_19', ['20240405174_good'], [])\n",
      "('../input/results/iconqa/iconqa_19/20240405174_good', ['result'], ['log.txt', 'iconqa_19.log'])\n",
      "('../input/results/iconqa/iconqa_19/20240405174_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_16', ['20240405174_good'], [])\n",
      "('../input/results/iconqa/iconqa_16/20240405174_good', ['result'], ['log.txt', 'iconqa_16.log'])\n",
      "('../input/results/iconqa/iconqa_16/20240405174_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_20', ['20240405174_good'], [])\n",
      "('../input/results/iconqa/iconqa_20/20240405174_good', ['result'], ['log.txt', 'iconqa_20.log'])\n",
      "('../input/results/iconqa/iconqa_20/20240405174_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_27', ['20240405174_good'], [])\n",
      "('../input/results/iconqa/iconqa_27/20240405174_good', ['result'], ['log.txt', 'iconqa_27.log'])\n",
      "('../input/results/iconqa/iconqa_27/20240405174_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_31', ['20240405174_good'], [])\n",
      "('../input/results/iconqa/iconqa_31/20240405174_good', ['result'], ['log.txt', 'iconqa_31.log'])\n",
      "('../input/results/iconqa/iconqa_31/20240405174_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_32', ['20240405174_good'], [])\n",
      "('../input/results/iconqa/iconqa_32/20240405174_good', ['result'], ['log.txt', 'iconqa_32.log'])\n",
      "('../input/results/iconqa/iconqa_32/20240405174_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_23', ['20240406041_good'], [])\n",
      "('../input/results/iconqa/iconqa_23/20240406041_good', ['result'], ['log.txt', 'iconqa_23.log'])\n",
      "('../input/results/iconqa/iconqa_23/20240406041_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_24', ['20240406042_good'], [])\n",
      "('../input/results/iconqa/iconqa_24/20240406042_good', ['result'], ['log.txt', 'iconqa_24.log'])\n",
      "('../input/results/iconqa/iconqa_24/20240406042_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n",
      "('../input/results/iconqa/iconqa_28', ['20240406054_good'], [])\n",
      "('../input/results/iconqa/iconqa_28/20240406054_good', ['result'], ['log.txt', 'iconqa_28.log'])\n",
      "('../input/results/iconqa/iconqa_28/20240406054_good/result', [], ['test_iconqa_result.json', 'test_iconqa_result_rank0.json', 'val_iconqa_result.json', 'val_iconqa_result_rank0.json'])\n"
     ]
    }
   ],
   "source": [
    "paths = os.walk(f'../input/results/{dataset}')\n",
    "paths\n",
    "\n",
    "for path in paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_training_time(lines):\n",
    "    total_sec = 0\n",
    "    pattern = re.compile(r'Train: data epoch: \\[\\d+\\] Total time:')\n",
    "    sub_lines = [pattern.sub('', x) for x in lines if pattern.match(x)]\n",
    "    sub_lines = [x.strip().split(' ')[0] for x in sub_lines]\n",
    "    for time_str in sub_lines:\n",
    "        h ,m, s = [int(x) for x in time_str.split(':')]\n",
    "        total_sec = total_sec + s + m*60 + h*3600\n",
    "    return total_sec/3600\n",
    "\n",
    "def get_training_time_per_it(lines):\n",
    "    total_sec = []\n",
    "    pattern = re.compile(r'Train: data epoch: \\[\\d+\\] Total time:')\n",
    "    sub_lines = [pattern.sub('', x) for x in lines if pattern.match(x)]\n",
    "    sub_lines = [x.strip().split(' ')[1] for x in sub_lines]\n",
    "    for time_str in sub_lines:\n",
    "        sec = float(time_str.replace('(', ''))\n",
    "        total_sec.append(sec)\n",
    "    return sum(total_sec) / len(total_sec) \n",
    "\n",
    "def get_total_eval_time(lines):\n",
    "    total_sec = 0\n",
    "    pattern = re.compile(r'Evaluation Total time:')\n",
    "    sub_lines = [pattern.sub('', x) for x in lines if pattern.match(x)]\n",
    "    sub_lines = [x.strip().split(' ')[0] for x in sub_lines]\n",
    "    for time_str in sub_lines:\n",
    "        h ,m, s = [int(x) for x in time_str.split(':')]\n",
    "        total_sec = total_sec + s + m*60 + h*3600\n",
    "    return total_sec/3600\n",
    "\n",
    "def get_llm_trainable_params(lines):\n",
    "    # line example: trainable params: 1,032,192 || all params: 2,850,789,376 || trainable%: 0.03620723469400217\n",
    "    sub_lines = [x.lower() for x in lines if x.lower().startswith('trainable params: ')] \n",
    "    sub_lines = [x.split(' ')[2] for x in sub_lines]\n",
    "    if len(sub_lines) > 0:\n",
    "        return int(sub_lines[0].replace(',', ''))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_qformer_trainable_params(lines):\n",
    "    sub_lines = [x.lower() for x in lines if x.lower().startswith('number of trainable parameters in qformer:')] \n",
    "    sub_lines = [x.split(' ')[-1] for x in sub_lines]\n",
    "    if len(sub_lines) > 0:\n",
    "        return int(sub_lines[0].replace(',', ''))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_loss_hist(lines):\n",
    "    sub_lines = [x.lower() for x in lines if 'loss: ' in x.lower()] \n",
    "    sub_lines = [(x.split('loss: ')[1]).split(' ')[0] for x in sub_lines]\n",
    "    sub_lines = [float(x) for x in sub_lines]\n",
    "    return sub_lines\n",
    "\n",
    "\n",
    "def get_loss_per_epoch(lines):\n",
    "    # example: {\"train_lr\": \"0.000\", \"train_loss\": \"0.102\"}\n",
    "    sub_lines = [x.lower() for x in lines if '\"train_loss\":' in x.lower()]\n",
    "    sub_lines = [(x.replace('\"', '').replace('}', '').split(' ')[3]) for x in sub_lines]\n",
    "    sub_lines = [float(x) for x in sub_lines]\n",
    "    return sub_lines\n",
    "\n",
    "def get_val_acc_per_epoch(lines):\n",
    "    # example: {\"val_agg_metrics\": 77.30090605627086, \"val_acc\": 77.30090605627086, \"val_best_epoch\": 0}\n",
    "    sub_lines = [x.lower() for x in lines if '\"val_acc\":' in x.lower()]\n",
    "    sub_lines = [(x.replace('\"', '').replace(',', '').split(' ')[3]) for x in sub_lines]\n",
    "    sub_lines = [float(x) for x in sub_lines]\n",
    "    return sub_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = os.walk(f'../input/results/{dataset}')\n",
    "\n",
    "result_list = []\n",
    "for path in paths:\n",
    "    sub_dir = path[0]\n",
    "    files = path[2]\n",
    "    if sub_dir.endswith('_good'):\n",
    "        test_result_file = f'test_{dataset}_result.json'\n",
    "        val_result_file = f'val_{dataset}_result.json'\n",
    "        experiment_id = (sub_dir.split('/')[4]).split('_')[1]\n",
    "        \n",
    "        test_df = pd.read_json(f'{sub_dir}/result/{test_result_file}')\n",
    "        test_df['correct'] = test_df['pred_ans'] == test_df['gt_ans']\n",
    "        test_accuracy = test_df['correct'].sum()/test_df.shape[0]\n",
    "        # save the wrong questions\n",
    "        test_df[test_df['correct'] == False].to_csv(f'{sub_dir}/result/wrong_questions.csv', index=False)\n",
    "\n",
    "        val_df = pd.read_json(f'{sub_dir}/result/{val_result_file}')\n",
    "        val_df['correct'] = val_df['pred_ans'] == val_df['gt_ans']\n",
    "        val_accuracy = val_df['correct'].sum()/val_df.shape[0]\n",
    "    \n",
    "        log_file = [x for x in files if x.endswith('.log')][0]\n",
    "        with open(f'{sub_dir}/{log_file}') as f:\n",
    "            lines = f.readlines()\n",
    "            training_time = get_total_training_time(lines)\n",
    "            time_per_it = get_training_time_per_it(lines)\n",
    "            eval_time = get_total_eval_time(lines)\n",
    "            llm_trainable_params = get_llm_trainable_params(lines)\n",
    "            qformer_trainable_params = get_qformer_trainable_params(lines)\n",
    "            loss_hist = get_loss_hist(lines)\n",
    "        with open(f'{sub_dir}/log.txt') as f:\n",
    "            lines = f.readlines()\n",
    "            loss_per_epoch = get_loss_per_epoch(lines)\n",
    "            val_acc_per_epoch = get_val_acc_per_epoch(lines)\n",
    "\n",
    "        \n",
    "        result_list.append({\n",
    "            'experiment_id': int(experiment_id),\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'training_time': training_time,\n",
    "            'time_per_it': time_per_it,\n",
    "            'eval_time': eval_time,\n",
    "            'llm_trainable_params': llm_trainable_params,\n",
    "            'qformer_trainable_params': qformer_trainable_params,\n",
    "            'loss_hist': loss_hist,\n",
    "            'loss_per_epoch': loss_per_epoch,\n",
    "            'val_acc_per_epoch': val_acc_per_epoch\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>training_time</th>\n",
       "      <th>time_per_it</th>\n",
       "      <th>eval_time</th>\n",
       "      <th>llm_trainable_params</th>\n",
       "      <th>qformer_trainable_params</th>\n",
       "      <th>loss_hist</th>\n",
       "      <th>loss_per_epoch</th>\n",
       "      <th>val_acc_per_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.742717</td>\n",
       "      <td>0.723243</td>\n",
       "      <td>5.903056</td>\n",
       "      <td>0.408009</td>\n",
       "      <td>2.597778</td>\n",
       "      <td>1032192</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.6991, 0.3073, 0.1862, 0.107, 0.3492, 0.2191...</td>\n",
       "      <td>[0.154, 0.116, 0.098, 0.089, 0.082, 0.079, 0.0...</td>\n",
       "      <td>[66.03863204559848, 68.84103863204561, 68.5877...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.739234</td>\n",
       "      <td>0.722926</td>\n",
       "      <td>5.466667</td>\n",
       "      <td>0.415620</td>\n",
       "      <td>2.360833</td>\n",
       "      <td>2064384</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7105, 0.3305, 0.236, 0.1027, 0.391, 0.2135,...</td>\n",
       "      <td>[0.156, 0.113, 0.096, 0.087, 0.081, 0.078, 0.0...</td>\n",
       "      <td>[67.19442685243826, 68.1760607979734, 69.49018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>0.753167</td>\n",
       "      <td>0.746992</td>\n",
       "      <td>8.102500</td>\n",
       "      <td>0.410707</td>\n",
       "      <td>3.463056</td>\n",
       "      <td>4128768</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7188, 0.2298, 0.1928, 0.1648, 0.3075, 0.207...</td>\n",
       "      <td>[0.153, 0.115, 0.1, 0.09, 0.082, 0.079, 0.075,...</td>\n",
       "      <td>[66.07029765674477, 66.2444585180494, 67.47941...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0.729417</td>\n",
       "      <td>0.733851</td>\n",
       "      <td>4.274722</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>1.930278</td>\n",
       "      <td>8257536</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7184, 0.2349, 0.2304, 0.1273, 0.3047, 0.242...</td>\n",
       "      <td>[0.156, 0.117, 0.099, 0.088, 0.082, 0.077, 0.0...</td>\n",
       "      <td>[66.2444585180494, 67.30525649145028, 70.55098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.747625</td>\n",
       "      <td>0.725617</td>\n",
       "      <td>8.079167</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>3.406667</td>\n",
       "      <td>589824</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7223, 0.2497, 0.2203, 0.1119, 0.2821, 0.225...</td>\n",
       "      <td>[0.159, 0.117, 0.1, 0.09, 0.084, 0.079, 0.076,...</td>\n",
       "      <td>[63.01456618112729, 67.32108929702343, 68.0018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.729101</td>\n",
       "      <td>0.718493</td>\n",
       "      <td>5.465000</td>\n",
       "      <td>0.415510</td>\n",
       "      <td>2.347222</td>\n",
       "      <td>1179648</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.704, 0.2716, 0.1871, 0.1251, 0.3121, 0.2337...</td>\n",
       "      <td>[0.155, 0.114, 0.097, 0.088, 0.081, 0.076, 0.0...</td>\n",
       "      <td>[65.83280557314755, 67.59024699176695, 69.5851...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>0.743509</td>\n",
       "      <td>0.735592</td>\n",
       "      <td>6.895278</td>\n",
       "      <td>0.403292</td>\n",
       "      <td>2.928056</td>\n",
       "      <td>2359296</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7194, 0.2532, 0.2272, 0.1145, 0.3076, 0.241...</td>\n",
       "      <td>[0.154, 0.113, 0.094, 0.087, 0.082, 0.077, 0.0...</td>\n",
       "      <td>[64.70867637745408, 67.25775807473084, 68.2552...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>0.727834</td>\n",
       "      <td>0.714218</td>\n",
       "      <td>5.869722</td>\n",
       "      <td>0.405700</td>\n",
       "      <td>2.586389</td>\n",
       "      <td>4718592</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7105, 0.2472, 0.2146, 0.1434, 0.282, 0.2245...</td>\n",
       "      <td>[0.159, 0.117, 0.1, 0.091, 0.084, 0.079, 0.076...</td>\n",
       "      <td>[65.84863837872071, 67.2102596580114, 68.90436...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.745408</td>\n",
       "      <td>0.737017</td>\n",
       "      <td>7.210000</td>\n",
       "      <td>0.456825</td>\n",
       "      <td>3.149722</td>\n",
       "      <td>1622016</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7395, 0.2336, 0.2247, 0.1168, 0.3086, 0.212...</td>\n",
       "      <td>[0.158, 0.115, 0.099, 0.088, 0.082, 0.079, 0.0...</td>\n",
       "      <td>[66.95693476884104, 66.35528815706144, 70.4243...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.757283</td>\n",
       "      <td>0.752850</td>\n",
       "      <td>9.097778</td>\n",
       "      <td>0.461147</td>\n",
       "      <td>3.838333</td>\n",
       "      <td>3244032</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7194, 0.264, 0.1849, 0.1432, 0.3459, 0.2349...</td>\n",
       "      <td>[0.151, 0.112, 0.095, 0.087, 0.08, 0.077, 0.07...</td>\n",
       "      <td>[67.79607346421786, 68.99936668777707, 68.8568...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11</td>\n",
       "      <td>0.730684</td>\n",
       "      <td>0.729576</td>\n",
       "      <td>4.848056</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>2.215000</td>\n",
       "      <td>6488064</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7024, 0.2807, 0.2094, 0.1008, 0.3577, 0.221...</td>\n",
       "      <td>[0.151, 0.114, 0.097, 0.089, 0.082, 0.077, 0.0...</td>\n",
       "      <td>[66.56111462951235, 68.09689677010766, 71.1526...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>0.738442</td>\n",
       "      <td>0.721184</td>\n",
       "      <td>8.868056</td>\n",
       "      <td>0.449533</td>\n",
       "      <td>3.845278</td>\n",
       "      <td>12976128</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7242, 0.2262, 0.1775, 0.1326, 0.3464, 0.219...</td>\n",
       "      <td>[0.158, 0.115, 0.099, 0.089, 0.083, 0.077, 0.0...</td>\n",
       "      <td>[65.880303989867, 67.97023432552248, 68.936035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13</td>\n",
       "      <td>0.714693</td>\n",
       "      <td>0.704718</td>\n",
       "      <td>7.754167</td>\n",
       "      <td>0.393020</td>\n",
       "      <td>3.136111</td>\n",
       "      <td>0</td>\n",
       "      <td>184320</td>\n",
       "      <td>[0.7004, 0.2161, 0.1802, 0.1251, 0.3446, 0.211...</td>\n",
       "      <td>[0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[65.95946801773275, 65.94363521215959, 66.3711...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14</td>\n",
       "      <td>0.707093</td>\n",
       "      <td>0.700918</td>\n",
       "      <td>7.965556</td>\n",
       "      <td>0.403767</td>\n",
       "      <td>3.174444</td>\n",
       "      <td>0</td>\n",
       "      <td>368640</td>\n",
       "      <td>[0.7004, 0.2161, 0.1802, 0.1251, 0.3446, 0.211...</td>\n",
       "      <td>[0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[65.95946801773275, 65.94363521215959, 66.3711...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15</td>\n",
       "      <td>0.680494</td>\n",
       "      <td>0.664661</td>\n",
       "      <td>5.693056</td>\n",
       "      <td>0.393509</td>\n",
       "      <td>2.368333</td>\n",
       "      <td>0</td>\n",
       "      <td>737280</td>\n",
       "      <td>[0.7004, 0.2036, 0.1702, 0.1237, 0.2714, 0.211...</td>\n",
       "      <td>[0.187, 0.154, 0.141, 0.136, 0.131, 0.125, 0.1...</td>\n",
       "      <td>[60.76630778974034, 63.220392653578216, 64.075...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16</td>\n",
       "      <td>0.707093</td>\n",
       "      <td>0.700918</td>\n",
       "      <td>7.958056</td>\n",
       "      <td>0.403387</td>\n",
       "      <td>3.164722</td>\n",
       "      <td>0</td>\n",
       "      <td>1474560</td>\n",
       "      <td>[0.7004, 0.2161, 0.1802, 0.1251, 0.3446, 0.211...</td>\n",
       "      <td>[0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[65.95946801773275, 65.94363521215959, 66.3711...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>0.722293</td>\n",
       "      <td>0.708676</td>\n",
       "      <td>7.259444</td>\n",
       "      <td>0.367960</td>\n",
       "      <td>3.083611</td>\n",
       "      <td>0</td>\n",
       "      <td>36864</td>\n",
       "      <td>[0.7124, 0.2317, 0.1909, 0.1257, 0.3457, 0.203...</td>\n",
       "      <td>[0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[64.56618112729575, 66.5769474350855, 66.37112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.722293</td>\n",
       "      <td>0.708676</td>\n",
       "      <td>7.302778</td>\n",
       "      <td>0.370167</td>\n",
       "      <td>3.083889</td>\n",
       "      <td>0</td>\n",
       "      <td>73728</td>\n",
       "      <td>[0.7124, 0.2317, 0.1909, 0.1257, 0.3457, 0.203...</td>\n",
       "      <td>[0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[64.56618112729575, 66.5769474350855, 66.37112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19</td>\n",
       "      <td>0.722293</td>\n",
       "      <td>0.708676</td>\n",
       "      <td>7.370556</td>\n",
       "      <td>0.373607</td>\n",
       "      <td>3.145000</td>\n",
       "      <td>0</td>\n",
       "      <td>147456</td>\n",
       "      <td>[0.7124, 0.2317, 0.1909, 0.1257, 0.3457, 0.203...</td>\n",
       "      <td>[0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[64.56618112729575, 66.5769474350855, 66.37112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20</td>\n",
       "      <td>0.722293</td>\n",
       "      <td>0.708676</td>\n",
       "      <td>7.468889</td>\n",
       "      <td>0.378573</td>\n",
       "      <td>3.162778</td>\n",
       "      <td>0</td>\n",
       "      <td>294912</td>\n",
       "      <td>[0.7124, 0.2317, 0.1909, 0.1257, 0.3457, 0.203...</td>\n",
       "      <td>[0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[64.56618112729575, 66.5769474350855, 66.37112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0.704085</td>\n",
       "      <td>0.693635</td>\n",
       "      <td>7.250278</td>\n",
       "      <td>0.367533</td>\n",
       "      <td>3.094722</td>\n",
       "      <td>0</td>\n",
       "      <td>44544</td>\n",
       "      <td>[0.7435, 0.21, 0.1951, 0.1472, 0.334, 0.2068, ...</td>\n",
       "      <td>[0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...</td>\n",
       "      <td>[63.23622545915136, 53.79987333755542, 64.9620...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0.704085</td>\n",
       "      <td>0.693635</td>\n",
       "      <td>7.287500</td>\n",
       "      <td>0.369407</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>0</td>\n",
       "      <td>89088</td>\n",
       "      <td>[0.7435, 0.21, 0.1951, 0.1472, 0.334, 0.2068, ...</td>\n",
       "      <td>[0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...</td>\n",
       "      <td>[63.23622545915136, 53.79987333755542, 64.9620...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23</td>\n",
       "      <td>0.704085</td>\n",
       "      <td>0.693635</td>\n",
       "      <td>7.228056</td>\n",
       "      <td>0.366360</td>\n",
       "      <td>3.082222</td>\n",
       "      <td>0</td>\n",
       "      <td>178176</td>\n",
       "      <td>[0.7435, 0.21, 0.1951, 0.1472, 0.334, 0.2068, ...</td>\n",
       "      <td>[0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...</td>\n",
       "      <td>[63.23622545915136, 53.79987333755542, 64.9620...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>24</td>\n",
       "      <td>0.704085</td>\n",
       "      <td>0.693635</td>\n",
       "      <td>7.354722</td>\n",
       "      <td>0.372793</td>\n",
       "      <td>3.085833</td>\n",
       "      <td>0</td>\n",
       "      <td>356352</td>\n",
       "      <td>[0.7435, 0.21, 0.1951, 0.1472, 0.334, 0.2068, ...</td>\n",
       "      <td>[0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...</td>\n",
       "      <td>[63.23622545915136, 53.79987333755542, 64.9620...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>0.709310</td>\n",
       "      <td>0.709151</td>\n",
       "      <td>8.758333</td>\n",
       "      <td>0.443927</td>\n",
       "      <td>3.138611</td>\n",
       "      <td>0</td>\n",
       "      <td>265728</td>\n",
       "      <td>[0.7335, 0.2075, 0.1997, 0.1348, 0.3379, 0.247...</td>\n",
       "      <td>[0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...</td>\n",
       "      <td>[65.12032932235591, 66.8777707409753, 67.84357...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26</td>\n",
       "      <td>0.672419</td>\n",
       "      <td>0.667036</td>\n",
       "      <td>4.742778</td>\n",
       "      <td>0.450738</td>\n",
       "      <td>1.768333</td>\n",
       "      <td>0</td>\n",
       "      <td>531456</td>\n",
       "      <td>[0.7335, 0.2081, 0.1781, 0.1402, 0.2864, 0.209...</td>\n",
       "      <td>[0.186, 0.155, 0.142, 0.135, 0.13, 0.125, 0.12...</td>\n",
       "      <td>[60.148828372387584, 63.91703609879671, 63.790...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.709310</td>\n",
       "      <td>0.709151</td>\n",
       "      <td>8.865556</td>\n",
       "      <td>0.449347</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0</td>\n",
       "      <td>1062912</td>\n",
       "      <td>[0.7335, 0.2075, 0.1997, 0.1348, 0.3379, 0.247...</td>\n",
       "      <td>[0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...</td>\n",
       "      <td>[65.12032932235591, 66.8777707409753, 67.84357...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>28</td>\n",
       "      <td>0.709310</td>\n",
       "      <td>0.709151</td>\n",
       "      <td>8.618889</td>\n",
       "      <td>0.436893</td>\n",
       "      <td>3.055833</td>\n",
       "      <td>0</td>\n",
       "      <td>2125824</td>\n",
       "      <td>[0.7335, 0.2075, 0.1997, 0.1348, 0.3379, 0.247...</td>\n",
       "      <td>[0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...</td>\n",
       "      <td>[65.12032932235591, 66.8777707409753, 67.84357...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29</td>\n",
       "      <td>0.729259</td>\n",
       "      <td>0.717384</td>\n",
       "      <td>10.492222</td>\n",
       "      <td>0.531807</td>\n",
       "      <td>3.883333</td>\n",
       "      <td>1622016</td>\n",
       "      <td>265728</td>\n",
       "      <td>[0.7279, 0.1998, 0.195, 0.1487, 0.3211, 0.1427...</td>\n",
       "      <td>[0.155, 0.129, 0.12, 0.113, 0.11, 0.103, 0.1, ...</td>\n",
       "      <td>[65.83280557314755, 64.66117796073463, 65.5636...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>30</td>\n",
       "      <td>0.717543</td>\n",
       "      <td>0.710418</td>\n",
       "      <td>10.784722</td>\n",
       "      <td>0.546613</td>\n",
       "      <td>4.024167</td>\n",
       "      <td>3244032</td>\n",
       "      <td>531456</td>\n",
       "      <td>[0.7283, 0.1762, 0.2186, 0.1432, 0.2765, 0.169...</td>\n",
       "      <td>[0.154, 0.131, 0.118, 0.237, 0.105, 0.101, 0.0...</td>\n",
       "      <td>[65.50031665611147, 64.48701709943002, 63.5845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31</td>\n",
       "      <td>0.711051</td>\n",
       "      <td>0.709626</td>\n",
       "      <td>10.991389</td>\n",
       "      <td>0.557087</td>\n",
       "      <td>4.036667</td>\n",
       "      <td>6488064</td>\n",
       "      <td>1062912</td>\n",
       "      <td>[0.7213, 0.1949, 0.1685, 0.1297, 0.2959, 0.191...</td>\n",
       "      <td>[0.155, 0.126, 0.117, 0.111, 0.104, 0.099, 0.0...</td>\n",
       "      <td>[62.127929069031026, 63.42621912602914, 65.373...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32</td>\n",
       "      <td>0.720868</td>\n",
       "      <td>0.716276</td>\n",
       "      <td>10.928056</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>4.033611</td>\n",
       "      <td>12976128</td>\n",
       "      <td>2125824</td>\n",
       "      <td>[0.7116, 0.1824, 0.1911, 0.1605, 0.3502, 0.148...</td>\n",
       "      <td>[0.154, 0.13, 0.123, 0.112, 0.107, 0.101, 0.09...</td>\n",
       "      <td>[62.365421152628244, 62.55541481950602, 67.463...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment_id  test_accuracy  val_accuracy  training_time  time_per_it  \\\n",
       "0               1       0.742717      0.723243       5.903056     0.408009   \n",
       "2               2       0.739234      0.722926       5.466667     0.415620   \n",
       "19              3       0.753167      0.746992       8.102500     0.410707   \n",
       "13              4       0.729417      0.733851       4.274722     0.406250   \n",
       "5               5       0.747625      0.725617       8.079167     0.409500   \n",
       "4               6       0.729101      0.718493       5.465000     0.415510   \n",
       "20              7       0.743509      0.735592       6.895278     0.403292   \n",
       "14              8       0.727834      0.714218       5.869722     0.405700   \n",
       "9               9       0.745408      0.737017       7.210000     0.456825   \n",
       "8              10       0.757283      0.752850       9.097778     0.461147   \n",
       "21             11       0.730684      0.729576       4.848056     0.460800   \n",
       "15             12       0.738442      0.721184       8.868056     0.449533   \n",
       "17             13       0.714693      0.704718       7.754167     0.393020   \n",
       "16             14       0.707093      0.700918       7.965556     0.403767   \n",
       "22             15       0.680494      0.664661       5.693056     0.393509   \n",
       "24             16       0.707093      0.700918       7.958056     0.403387   \n",
       "1              17       0.722293      0.708676       7.259444     0.367960   \n",
       "3              18       0.722293      0.708676       7.302778     0.370167   \n",
       "23             19       0.722293      0.708676       7.370556     0.373607   \n",
       "25             20       0.722293      0.708676       7.468889     0.378573   \n",
       "6              21       0.704085      0.693635       7.250278     0.367533   \n",
       "7              22       0.704085      0.693635       7.287500     0.369407   \n",
       "29             23       0.704085      0.693635       7.228056     0.366360   \n",
       "30             24       0.704085      0.693635       7.354722     0.372793   \n",
       "10             25       0.709310      0.709151       8.758333     0.443927   \n",
       "11             26       0.672419      0.667036       4.742778     0.450738   \n",
       "26             27       0.709310      0.709151       8.865556     0.449347   \n",
       "31             28       0.709310      0.709151       8.618889     0.436893   \n",
       "18             29       0.729259      0.717384      10.492222     0.531807   \n",
       "12             30       0.717543      0.710418      10.784722     0.546613   \n",
       "27             31       0.711051      0.709626      10.991389     0.557087   \n",
       "28             32       0.720868      0.716276      10.928056     0.553900   \n",
       "\n",
       "    eval_time  llm_trainable_params  qformer_trainable_params  \\\n",
       "0    2.597778               1032192                         0   \n",
       "2    2.360833               2064384                         0   \n",
       "19   3.463056               4128768                         0   \n",
       "13   1.930278               8257536                         0   \n",
       "5    3.406667                589824                         0   \n",
       "4    2.347222               1179648                         0   \n",
       "20   2.928056               2359296                         0   \n",
       "14   2.586389               4718592                         0   \n",
       "9    3.149722               1622016                         0   \n",
       "8    3.838333               3244032                         0   \n",
       "21   2.215000               6488064                         0   \n",
       "15   3.845278              12976128                         0   \n",
       "17   3.136111                     0                    184320   \n",
       "16   3.174444                     0                    368640   \n",
       "22   2.368333                     0                    737280   \n",
       "24   3.164722                     0                   1474560   \n",
       "1    3.083611                     0                     36864   \n",
       "3    3.083889                     0                     73728   \n",
       "23   3.145000                     0                    147456   \n",
       "25   3.162778                     0                    294912   \n",
       "6    3.094722                     0                     44544   \n",
       "7    3.080000                     0                     89088   \n",
       "29   3.082222                     0                    178176   \n",
       "30   3.085833                     0                    356352   \n",
       "10   3.138611                     0                    265728   \n",
       "11   1.768333                     0                    531456   \n",
       "26   3.180000                     0                   1062912   \n",
       "31   3.055833                     0                   2125824   \n",
       "18   3.883333               1622016                    265728   \n",
       "12   4.024167               3244032                    531456   \n",
       "27   4.036667               6488064                   1062912   \n",
       "28   4.033611              12976128                   2125824   \n",
       "\n",
       "                                            loss_hist  \\\n",
       "0   [0.6991, 0.3073, 0.1862, 0.107, 0.3492, 0.2191...   \n",
       "2   [0.7105, 0.3305, 0.236, 0.1027, 0.391, 0.2135,...   \n",
       "19  [0.7188, 0.2298, 0.1928, 0.1648, 0.3075, 0.207...   \n",
       "13  [0.7184, 0.2349, 0.2304, 0.1273, 0.3047, 0.242...   \n",
       "5   [0.7223, 0.2497, 0.2203, 0.1119, 0.2821, 0.225...   \n",
       "4   [0.704, 0.2716, 0.1871, 0.1251, 0.3121, 0.2337...   \n",
       "20  [0.7194, 0.2532, 0.2272, 0.1145, 0.3076, 0.241...   \n",
       "14  [0.7105, 0.2472, 0.2146, 0.1434, 0.282, 0.2245...   \n",
       "9   [0.7395, 0.2336, 0.2247, 0.1168, 0.3086, 0.212...   \n",
       "8   [0.7194, 0.264, 0.1849, 0.1432, 0.3459, 0.2349...   \n",
       "21  [0.7024, 0.2807, 0.2094, 0.1008, 0.3577, 0.221...   \n",
       "15  [0.7242, 0.2262, 0.1775, 0.1326, 0.3464, 0.219...   \n",
       "17  [0.7004, 0.2161, 0.1802, 0.1251, 0.3446, 0.211...   \n",
       "16  [0.7004, 0.2161, 0.1802, 0.1251, 0.3446, 0.211...   \n",
       "22  [0.7004, 0.2036, 0.1702, 0.1237, 0.2714, 0.211...   \n",
       "24  [0.7004, 0.2161, 0.1802, 0.1251, 0.3446, 0.211...   \n",
       "1   [0.7124, 0.2317, 0.1909, 0.1257, 0.3457, 0.203...   \n",
       "3   [0.7124, 0.2317, 0.1909, 0.1257, 0.3457, 0.203...   \n",
       "23  [0.7124, 0.2317, 0.1909, 0.1257, 0.3457, 0.203...   \n",
       "25  [0.7124, 0.2317, 0.1909, 0.1257, 0.3457, 0.203...   \n",
       "6   [0.7435, 0.21, 0.1951, 0.1472, 0.334, 0.2068, ...   \n",
       "7   [0.7435, 0.21, 0.1951, 0.1472, 0.334, 0.2068, ...   \n",
       "29  [0.7435, 0.21, 0.1951, 0.1472, 0.334, 0.2068, ...   \n",
       "30  [0.7435, 0.21, 0.1951, 0.1472, 0.334, 0.2068, ...   \n",
       "10  [0.7335, 0.2075, 0.1997, 0.1348, 0.3379, 0.247...   \n",
       "11  [0.7335, 0.2081, 0.1781, 0.1402, 0.2864, 0.209...   \n",
       "26  [0.7335, 0.2075, 0.1997, 0.1348, 0.3379, 0.247...   \n",
       "31  [0.7335, 0.2075, 0.1997, 0.1348, 0.3379, 0.247...   \n",
       "18  [0.7279, 0.1998, 0.195, 0.1487, 0.3211, 0.1427...   \n",
       "12  [0.7283, 0.1762, 0.2186, 0.1432, 0.2765, 0.169...   \n",
       "27  [0.7213, 0.1949, 0.1685, 0.1297, 0.2959, 0.191...   \n",
       "28  [0.7116, 0.1824, 0.1911, 0.1605, 0.3502, 0.148...   \n",
       "\n",
       "                                       loss_per_epoch  \\\n",
       "0   [0.154, 0.116, 0.098, 0.089, 0.082, 0.079, 0.0...   \n",
       "2   [0.156, 0.113, 0.096, 0.087, 0.081, 0.078, 0.0...   \n",
       "19  [0.153, 0.115, 0.1, 0.09, 0.082, 0.079, 0.075,...   \n",
       "13  [0.156, 0.117, 0.099, 0.088, 0.082, 0.077, 0.0...   \n",
       "5   [0.159, 0.117, 0.1, 0.09, 0.084, 0.079, 0.076,...   \n",
       "4   [0.155, 0.114, 0.097, 0.088, 0.081, 0.076, 0.0...   \n",
       "20  [0.154, 0.113, 0.094, 0.087, 0.082, 0.077, 0.0...   \n",
       "14  [0.159, 0.117, 0.1, 0.091, 0.084, 0.079, 0.076...   \n",
       "9   [0.158, 0.115, 0.099, 0.088, 0.082, 0.079, 0.0...   \n",
       "8   [0.151, 0.112, 0.095, 0.087, 0.08, 0.077, 0.07...   \n",
       "21  [0.151, 0.114, 0.097, 0.089, 0.082, 0.077, 0.0...   \n",
       "15  [0.158, 0.115, 0.099, 0.089, 0.083, 0.077, 0.0...   \n",
       "17  [0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "16  [0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "22  [0.187, 0.154, 0.141, 0.136, 0.131, 0.125, 0.1...   \n",
       "24  [0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "1   [0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "3   [0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "23  [0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "25  [0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "6   [0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...   \n",
       "7   [0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...   \n",
       "29  [0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...   \n",
       "30  [0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...   \n",
       "10  [0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...   \n",
       "11  [0.186, 0.155, 0.142, 0.135, 0.13, 0.125, 0.12...   \n",
       "26  [0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...   \n",
       "31  [0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...   \n",
       "18  [0.155, 0.129, 0.12, 0.113, 0.11, 0.103, 0.1, ...   \n",
       "12  [0.154, 0.131, 0.118, 0.237, 0.105, 0.101, 0.0...   \n",
       "27  [0.155, 0.126, 0.117, 0.111, 0.104, 0.099, 0.0...   \n",
       "28  [0.154, 0.13, 0.123, 0.112, 0.107, 0.101, 0.09...   \n",
       "\n",
       "                                    val_acc_per_epoch  \n",
       "0   [66.03863204559848, 68.84103863204561, 68.5877...  \n",
       "2   [67.19442685243826, 68.1760607979734, 69.49018...  \n",
       "19  [66.07029765674477, 66.2444585180494, 67.47941...  \n",
       "13  [66.2444585180494, 67.30525649145028, 70.55098...  \n",
       "5   [63.01456618112729, 67.32108929702343, 68.0018...  \n",
       "4   [65.83280557314755, 67.59024699176695, 69.5851...  \n",
       "20  [64.70867637745408, 67.25775807473084, 68.2552...  \n",
       "14  [65.84863837872071, 67.2102596580114, 68.90436...  \n",
       "9   [66.95693476884104, 66.35528815706144, 70.4243...  \n",
       "8   [67.79607346421786, 68.99936668777707, 68.8568...  \n",
       "21  [66.56111462951235, 68.09689677010766, 71.1526...  \n",
       "15  [65.880303989867, 67.97023432552248, 68.936035...  \n",
       "17  [65.95946801773275, 65.94363521215959, 66.3711...  \n",
       "16  [65.95946801773275, 65.94363521215959, 66.3711...  \n",
       "22  [60.76630778974034, 63.220392653578216, 64.075...  \n",
       "24  [65.95946801773275, 65.94363521215959, 66.3711...  \n",
       "1   [64.56618112729575, 66.5769474350855, 66.37112...  \n",
       "3   [64.56618112729575, 66.5769474350855, 66.37112...  \n",
       "23  [64.56618112729575, 66.5769474350855, 66.37112...  \n",
       "25  [64.56618112729575, 66.5769474350855, 66.37112...  \n",
       "6   [63.23622545915136, 53.79987333755542, 64.9620...  \n",
       "7   [63.23622545915136, 53.79987333755542, 64.9620...  \n",
       "29  [63.23622545915136, 53.79987333755542, 64.9620...  \n",
       "30  [63.23622545915136, 53.79987333755542, 64.9620...  \n",
       "10  [65.12032932235591, 66.8777707409753, 67.84357...  \n",
       "11  [60.148828372387584, 63.91703609879671, 63.790...  \n",
       "26  [65.12032932235591, 66.8777707409753, 67.84357...  \n",
       "31  [65.12032932235591, 66.8777707409753, 67.84357...  \n",
       "18  [65.83280557314755, 64.66117796073463, 65.5636...  \n",
       "12  [65.50031665611147, 64.48701709943002, 63.5845...  \n",
       "27  [62.127929069031026, 63.42621912602914, 65.373...  \n",
       "28  [62.365421152628244, 62.55541481950602, 67.463...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.sort_values(by=['experiment_id'])\n",
    "# result_df.sort_values(by=['training_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LLM LoRA (attn, FlanT5-XL)', 'r = 4')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_option_list = [\n",
    "    \"LLM LoRA (ffn, FlanT5-XL)             \",\n",
    "    \"LLM LoRA (attn, FlanT5-XL)            \",\n",
    "    \"LLM LoRA (all, FlanT5-XL)             \",\n",
    "    \"Q-Former LoRA (ffn, FlanT5-XL)        \",\n",
    "    \"Q-Former LoRA (self-attn, FlanT5-XL)  \",\n",
    "    \"Q-Former LoRA (cross-attn, FlanT5-XL) \",\n",
    "    \"Q-Former LoRA (all, FlanT5-XL)        \",\n",
    "    \"Q-Former and LLM LoRA (all, FlanT5-XL)\",\n",
    "    \"LLM LoRA (ffn, Vicuna-7B)             \",\n",
    "    \"LLM LoRA (attn, Vicuna-7B)            \",\n",
    "    \"LLM LoRA (all, Vicuna-7B)             \",\n",
    "    \"Q-Former LoRA (ffn, Vicuna-7B)        \",\n",
    "    \"Q-Former LoRA (self-attn, Vicuna-7B)  \",\n",
    "    \"Q-Former LoRA (cross-attn, Vicuna-7B) \",\n",
    "    \"Q-Former LoRA (all, Vicuna-7B)        \",\n",
    "    \"Q-Former and LLM LoRA (all, Vicuna-7B)\",\n",
    "    \"Q-Former (self-attn QVO, FlanT5-XL)   \",\n",
    "    \"Q-Former (cross-attn QVO, FlanT5-XL)  \",\n",
    "    \"Q-Former (cross-attn QV, FlanT5-XL)   \",\n",
    "]\n",
    "method_option_list = [x.strip() for x in method_option_list]\n",
    "\n",
    "r_option_list = [\n",
    "    \"r = 1\",\n",
    "    \"r = 2\",\n",
    "    \"r = 4\",\n",
    "    \"r = 8\",\n",
    "]\n",
    "\n",
    "def experiment_id_to_info(id):\n",
    "    row = math.floor((id-1)/4)\n",
    "    col = (id-1)%4\n",
    "    method = \"\"\n",
    "    r = 0\n",
    "\n",
    "    method_option = method_option_list[row]\n",
    "\n",
    "    r_option = r_option_list[col]\n",
    "\n",
    "    return (method_option, r_option)\n",
    "\n",
    "experiment_id_to_info(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93042/2001529989.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.6991, 0.3073, 0.1862, 0.107, 0.3492, 0.2191, 0.1775, 0.1711, 0.1743, 0.2838, 0.3249, 0.1213, 0.1005, 0.1618, 0.1851, 0.1774, 0.177, 0.2357, 0.1534, 0.138, 0.1422, 0.1951, 0.1165, 0.1824, 0.1343, 0.0693, 0.2148, 0.1615, 0.0941, 0.1355, 0.0926, 0.1492, 0.141, 0.508, 0.1396, 0.1068, 0.0748, 0.14, 0.118, 0.3363, 0.118, 0.2328, 0.1089, 0.142, 0.1336, 0.231, 0.0578, 0.2556, 0.2198, 0.1064, 0.1288, 0.1763, 0.0861, 0.1735, 0.0808, 0.3305, 0.1247, 0.2025, 0.1497, 0.1202, 0.1427, 0.1574, 0.0197, 0.0562, 0.1179, 0.0997, 0.1761, 0.1483, 0.0872, 0.0686, 0.1491, 0.0367, 0.2538, 0.0921, 0.1241, 0.1312, 0.0717, 0.1513, 0.0376, 0.1207, 0.1346, 0.089, 0.1905, 0.1751, 0.148, 0.0434, 0.1328, 0.2504, 0.0555, 0.0975, 0.0911, 0.2544, 0.02, 0.1466, 0.1598, 0.0306, 0.1544, 0.0545, 0.0939, 0.0633, 0.0755, 0.1267, 0.1378, 0.1686, 0.1139, 0.1741, 0.2346, 0.0943, 0.0814, 0.1232, 0.1528, 0.0897, 0.024, 0.1077, 0.0402, 0.2593, 0.1435, 0.1275, 0.268, 0.0259, 0.0777, 0.0338, 0.0164, 0.047, 0.122, 0.0547, 0.1786, 0.2064, 0.0696, 0.1242, 0.0597, 0.1513, 0.0517, 0.2031, 0.1776, 0.2347, 0.2676, 0.1246, 0.1082, 0.1367, 0.1731, 0.1214, 0.149, 0.0906, 0.0184, 0.1315, 0.1304, 0.0559, 0.1005, 0.049, 0.2062, 0.1445, 0.2729, 0.2287, 0.1935, 0.1233, 0.0282, 0.0639, 0.0037, 0.1278, 0.0279, 0.1082, 0.1538, 0.0499, 0.1828, 0.1344, 0.07, 0.1021, 0.2113, 0.1424, 0.2366, 0.0655, 0.0467, 0.1736, 0.0689, 0.1456, 0.0571, 0.2275, 0.1366, 0.0715, 0.0715, 0.3491, 0.1937, 0.0049, 0.114, 0.0514, 0.0821, 0.0677, 0.0736, 0.0692, 0.1351, 0.109, 0.0177, 0.1159, 0.1851, 0.0253, 0.0268, 0.0155, 0.0761, 0.0613, 0.081, 0.1478, 0.191, 0.0585, 0.0753, 0.1355, 0.0696, 0.0237, 0.0695, 0.0814, 0.2248, 0.1652, 0.0663, 0.0243, 0.0652, 0.1044, 0.0962, 0.0258, 0.1006, 0.0734, 0.1062, 0.0121, 0.0302, 0.0131, 0.0114, 0.1059, 0.0705, 0.1509, 0.0614, 0.1198, 0.0429, 0.0933, 0.0097, 0.0504, 0.0581, 0.0033, 0.1003, 0.0237, 0.0807, 0.0481, 0.0175, 0.0833, 0.118, 0.0069, 0.0599, 0.2099, 0.1372, 0.2376, 0.0467, 0.0782, 0.0835, 0.0635, 0.0618, 0.0362, 0.0858, 0.0708, 0.0094, 0.1147, 0.0066, 0.1331, 0.0319, 0.1071, 0.0561, 0.083, 0.0974, 0.0687, 0.0986, 0.14, 0.1106, 0.0788, 0.1197, 0.2061, 0.0485, 0.0353, 0.1481, 0.0673, 0.0224, 0.0448, 0.0545, 0.0195, 0.1716, 0.0903, 0.1168, 0.1042, 0.2407, 0.0641, 0.0226, 0.0827, 0.2354, 0.0524, 0.0984, 0.1004, 0.0276, 0.0597, 0.1431, 0.0579, 0.0441, 0.0383, 0.1218, 0.0697, 0.0457, 0.3352, 0.0276, 0.0565, 0.0821, 0.0237, 0.0204, 0.0836, 0.0949, 0.0909, 0.0603, 0.1452, 0.0611, 0.1514, 0.0648, 0.1668, 0.0615, 0.1057, 0.0913, 0.0588, 0.1448, 0.0916, 0.1978, 0.0209, 0.1383, 0.2888, 0.0727, 0.0776, 0.0879, 0.0249, 0.0914, 0.1803, 0.0788, 0.0575, 0.0563, 0.1075, 0.0247, 0.024, 0.1331, 0.1844, 0.0289, 0.0061, 0.0919, 0.0057, 0.1401, 0.0955, 0.2118, 0.0533, 0.1387, 0.2159, 0.0982, 0.201, 0.3338, 0.0648, 0.0648, 0.1731, 0.0992, 0.0968, 0.001, 0.0656, 0.2073, 0.0268, 0.0608, 0.1176, 0.0287, 0.1299, 0.0188, 0.1993, 0.1301, 0.1488, 0.049, 0.0238, 0.0807, 0.0266, 0.0301, 0.1283, 0.0607, 0.1181, 0.0213, 0.1538, 0.1185, 0.0147, 0.0347, 0.0983, 0.1065, 0.0001, 0.0985, 0.0886, 0.1996, 0.0012, 0.072, 0.0722, 0.1322, 0.0339, 0.0677, 0.0281, 0.0437, 0.0596, 0.0878, 0.0062, 0.0554, 0.0419, 0.0326, 0.1085, 0.099, 0.0665, 0.0723, 0.075, 0.1069, 0.0169, 0.102, 0.0889, 0.0588, 0.0232, 0.0537, 0.0801, 0.0564, 0.1078, 0.0166, 0.1068, 0.0698, 0.1135, 0.1144, 0.0263, 0.0703, 0.0479, 0.0207, 0.0503, 0.0406, 0.0001, 0.119, 0.072, 0.1021, 0.0005, 0.0019, 0.0326, 0.0219, 0.0337, 0.1019, 0.0068, 0.1612, 0.0124, 0.0355, 0.098, 0.1718, 0.2498, 0.0658, 0.0548, 0.0774, 0.1297, 0.1492, 0.0016, 0.22, 0.0575, 0.0461, 0.0431, 0.0085, 0.2267, 0.2066, 0.0182, 0.0299, 0.0666, 0.1134, 0.044, 0.0747, 0.2801, 0.0285, 0.0009, 0.061, 0.0583, 0.0634, 0.0014, 0.0865, 0.1873, 0.0072, 0.0362, 0.0847, 0.1389, 0.1615, 0.0107, 0.0329, 0.087, 0.1387, 0.0055, 0.0818, 0.135, 0.11, 0.1065, 0.0125, 0.119, 0.0, 0.0818, 0.0016, 0.0452, 0.0002, 0.0007, 0.0551, 0.0013, 0.0723, 0.1213, 0.1162, 0.0288, 0.0534, 0.0741, 0.2278, 0.001, 0.1002, 0.18, 0.041, 0.1415, 0.0255, 0.019, 0.1441, 0.1043, 0.0344, 0.1303, 0.1334, 0.0942, 0.0032, 0.0102, 0.0515, 0.0576, 0.234, 0.0172, 0.0615, 0.0205, 0.0009, 0.1067, 0.0469, 0.0897, 0.2951, 0.1307, 0.0874, 0.0138, 0.0908, 0.0456, 0.0576, 0.0556, 0.0971, 0.0647, 0.0964, 0.1541, 0.1035, 0.0402, 0.1027, 0.0462, 0.1315, 0.0249, 0.0438, 0.0, 0.0513, 0.1651, 0.0436, 0.0824, 0.1791, 0.0604, 0.104, 0.0172, 0.0719, 0.0265, 0.0205, 0.1008, 0.0708, 0.0083, 0.1277, 0.0856, 0.1318, 0.0406, 0.0021, 0.0978, 0.0318, 0.0328, 0.0029, 0.0005, 0.0606, 0.0002, 0.0403, 0.1854, 0.0853, 0.0106, 0.0768, 0.0789, 0.1732, 0.1658, 0.1096, 0.0692, 0.0089, 0.1673, 0.1049, 0.0842, 0.0308, 0.0471, 0.0008, 0.003, 0.104, 0.0705, 0.001, 0.0045, 0.0021, 0.0416, 0.0006, 0.0954, 0.0998, 0.007, 0.0698, 0.0176, 0.0494, 0.0227, 0.0713, 0.0647, 0.0785, 0.0485, 0.027, 0.0074, 0.1114, 0.0323, 0.1231, 0.1107, 0.0186, 0.0493, 0.0497, 0.1184, 0.094, 0.0412, 0.1443, 0.0019, 0.0523, 0.0364, 0.054, 0.0604, 0.1195, 0.0507, 0.0937, 0.063, 0.0116, 0.2945, 0.0022, 0.1475, 0.0706, 0.2157, 0.1744, 0.1983, 0.0766, 0.0307, 0.0215, 0.0871, 0.0664, 0.0055, 0.1177, 0.1035, 0.0254, 0.0058, 0.1176, 0.1018, 0.0269, 0.0613, 0.1312, 0.1829, 0.0005, 0.0017, 0.0295, 0.0379, 0.1184, 0.0527, 0.0753, 0.076, 0.084, 0.0489, 0.0617, 0.0322, 0.2311, 0.0078, 0.0727, 0.0876, 0.0855, 0.0602, 0.0857, 0.1141, 0.0739, 0.0003, 0.1673, 0.0547, 0.0799, 0.078, 0.1382, 0.0343, 0.0516, 0.0501, 0.0003, 0.0508, 0.121, 0.0045, 0.1342, 0.056, 0.1022, 0.1416, 0.063, 0.0604, 0.001, 0.0004, 0.0478, 0.0682, 0.0313, 0.001, 0.1323, 0.0907, 0.0032, 0.0858, 0.0074, 0.0741, 0.024, 0.2288, 0.0066, 0.0119, 0.1298, 0.0337, 0.1717, 0.0899, 0.0264, 0.0178, 0.0332, 0.1061, 0.162, 0.0667, 0.1521, 0.0797, 0.154, 0.0319, 0.0685, 0.1017, 0.0004, 0.0329, 0.1794, 0.034, 0.0369, 0.1108, 0.0177, 0.1046, 0.1141, 0.1006, 0.0732, 0.1341, 0.0151, 0.0584, 0.0397, 0.0, 0.0431, 0.0047, 0.1143, 0.0004, 0.0603, 0.0152, 0.1095, 0.0673, 0.1071, 0.0717, 0.0064, 0.1179, 0.0319, 0.1202, 0.3537, 0.0674, 0.0423, 0.0382, 0.0381, 0.0771, 0.0355, 0.0432, 0.0006, 0.0402, 0.0539, 0.0845, 0.2141, 0.0674, 0.0647, 0.0737, 0.3045, 0.065, 0.0006, 0.0079, 0.0121, 0.1108, 0.0856, 0.059, 0.0494, 0.1225, 0.017, 0.0554, 0.043, 0.1363, 0.0004, 0.062, 0.0454, 0.0519, 0.0299, 0.0808, 0.1657, 0.0032, 0.0016, 0.0454, 0.1222, 0.0589, 0.0001, 0.1027, 0.1383, 0.0212, 0.0001, 0.0325, 0.0237, 0.0003, 0.0432, 0.0813, 0.0907, 0.1131, 0.0287, 0.0719, 0.0093, 0.0042, 0.0704, 0.0013, 0.1494, 0.0696, 0.1099, 0.1777, 0.0039, 0.0404, 0.0682, 0.0031, 0.0487, 0.0858, 0.0796, 0.0191, 0.0296, 0.0157, 0.0311, 0.0443, 0.1183, 0.098, 0.0946, 0.073, 0.111, 0.1754, 0.0487, 0.0017, 0.0598, 0.0747, 0.0001, 0.0949, 0.0213, 0.0169, 0.0811, 0.0825, 0.0243, 0.0183, 0.1189, 0.0088, 0.0978, 0.0979, 0.1242, 0.0853, 0.023, 0.2716, 0.0565, 0.048, 0.0058, 0.0456, 0.0425, 0.0891, 0.1492, 0.0061, 0.1351, 0.0859, 0.0689, 0.0916, 0.0938, 0.0599, 0.0362, 0.0912, 0.1129, 0.0023, 0.1102, 0.057, 0.0789, 0.0828, 0.065, 0.3902, 0.0603, 0.049, 0.1122, 0.168, 0.1002, 0.0133, 0.0411, 0.0051, 0.083, 0.086, 0.0645, 0.0345, 0.0639, 0.0577, 0.008, 0.0401, 0.0895, 0.1032, 0.0423, 0.041, 0.1514, 0.0114, 0.0451, 0.1538, 0.0515, 0.1001, 0.0927, 0.0469, 0.0075, 0.0431, 0.128, 0.0695, 0.1039, 0.0351, 0.0469, 0.0812, 0.0003, 0.0485, 0.1021, 0.0387, 0.1176, 0.0833, 0.0354, 0.0099, 0.0628, 0.0401, 0.001, 0.253, 0.0194, 0.055, 0.1189, 0.117, 0.0011, 0.0004, 0.0032, 0.066, 0.12, 0.0638, 0.0384, 0.0312, 0.0871, 0.0857, 0.0592, 0.0007, 0.0658, 0.0208, 0.0006, 0.0356, 0.1873, 0.0301, 0.0573, 0.0278, 0.0648, 0.0692, 0.1742, 0.0721, 0.0001, 0.0032, 0.0455, 0.0814, 0.0134, 0.0929, 0.3127, 0.0665, 0.1795, 0.0004, 0.0585, 0.0515, 0.1853, 0.0636, 0.0331, 0.0873, 0.0205, 0.0327, 0.0275, 0.0655, 0.0123, 0.0744, 0.055, 0.0, 0.0013, 0.048, 0.0564, 0.0288, 0.0012, 0.0243, 0.0506, 0.0026, 0.0511, 0.1014, 0.1794, 0.0024, 0.0626, 0.0999, 0.0013, 0.0385, 0.1342, 0.0192, 0.4012, 0.0001, 0.0384, 0.0465, 0.0091, 0.2149, 0.1614, 0.0689, 0.0441, 0.0277, 0.0134, 0.0056, 0.1639, 0.2044, 0.1474, 0.01, 0.1427, 0.0646, 0.0423, 0.0169, 0.0124, 0.0001, 0.1014, 0.046, 0.1423, 0.043, 0.0449, 0.0517, 0.1039, 0.0759, 0.0717, 0.1659, 0.0907, 0.0463, 0.1586, 0.011, 0.0001, 0.0, 0.0408, 0.0738, 0.01, 0.0008, 0.0072, 0.0869, 0.0613, 0.0001, 0.0332, 0.1434, 0.1699, 0.0128, 0.0211, 0.0541, 0.1192, 0.0546, 0.0626, 0.0418, 0.0, 0.0188, 0.1047, 0.0004, 0.1008, 0.0624, 0.0641]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loss_hist_df.at[method_option, r_option] = str(result['loss_hist'])\n",
      "/tmp/ipykernel_93042/2001529989.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.154, 0.116, 0.098, 0.089, 0.082, 0.079, 0.074, 0.074, 0.069, 0.066, 0.064]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loss_per_epoch_df.at[method_option, r_option] = str(result['loss_per_epoch'])\n",
      "/tmp/ipykernel_93042/2001529989.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[66.03863204559848, 68.84103863204561, 68.58771374287524, 70.36098796706777, 70.01266624445852, 70.66181127295756, 71.99176694110196, 73.81253958201394, 72.83090563647878, 72.53008233058898, 72.32425585813806]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  val_acc_per_epoch_df.at[method_option, r_option] = str(result['val_acc_per_epoch'])\n",
      "/tmp/ipykernel_93042/2001529989.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.7105, 0.3305, 0.236, 0.1027, 0.391, 0.2135, 0.1565, 0.1608, 0.1788, 0.2602, 0.2595, 0.1267, 0.0988, 0.1929, 0.2515, 0.1676, 0.1461, 0.2238, 0.2174, 0.1175, 0.1425, 0.1949, 0.1135, 0.226, 0.1175, 0.0591, 0.2041, 0.1773, 0.1002, 0.1573, 0.1609, 0.147, 0.1207, 0.5619, 0.139, 0.1123, 0.0743, 0.1514, 0.1287, 0.335, 0.1219, 0.263, 0.1271, 0.2018, 0.1564, 0.2595, 0.0633, 0.2371, 0.3095, 0.1663, 0.0961, 0.1369, 0.1082, 0.1399, 0.089, 0.2659, 0.1495, 0.1614, 0.2234, 0.1666, 0.1164, 0.1546, 0.01, 0.0721, 0.1079, 0.0793, 0.2102, 0.1463, 0.0876, 0.07, 0.1828, 0.0455, 0.1634, 0.0767, 0.1303, 0.1147, 0.0809, 0.1168, 0.0504, 0.0543, 0.0951, 0.1755, 0.1802, 0.216, 0.1695, 0.0556, 0.1944, 0.2533, 0.0412, 0.108, 0.0761, 0.2679, 0.0435, 0.0715, 0.1596, 0.0282, 0.1557, 0.0599, 0.1045, 0.076, 0.0739, 0.2184, 0.1206, 0.156, 0.1678, 0.1659, 0.2204, 0.0558, 0.179, 0.1731, 0.1207, 0.0719, 0.0336, 0.1005, 0.0399, 0.0946, 0.1423, 0.1152, 0.2141, 0.0234, 0.0849, 0.064, 0.0506, 0.0597, 0.1688, 0.0782, 0.1682, 0.207, 0.1273, 0.1301, 0.0759, 0.1036, 0.032, 0.1606, 0.2003, 0.207, 0.0855, 0.1306, 0.1346, 0.1364, 0.2816, 0.1401, 0.171, 0.1029, 0.0111, 0.1052, 0.1291, 0.0551, 0.0928, 0.0703, 0.2233, 0.1042, 0.2325, 0.175, 0.4657, 0.1072, 0.0705, 0.1165, 0.073, 0.1198, 0.0398, 0.0854, 0.1458, 0.0138, 0.0591, 0.1417, 0.0896, 0.1054, 0.1544, 0.1378, 0.2222, 0.0735, 0.0586, 0.181, 0.0555, 0.1023, 0.0537, 0.1812, 0.1756, 0.113, 0.1228, 0.1772, 0.1912, 0.0028, 0.107, 0.0446, 0.1176, 0.0713, 0.0887, 0.0558, 0.1539, 0.0944, 0.0289, 0.1135, 0.0752, 0.0276, 0.1138, 0.0161, 0.1278, 0.1741, 0.0919, 0.1269, 0.0835, 0.0693, 0.0653, 0.1631, 0.0592, 0.0251, 0.048, 0.0678, 0.1891, 0.1439, 0.0506, 0.0023, 0.0394, 0.1286, 0.0808, 0.0472, 0.0936, 0.0283, 0.1232, 0.0177, 0.0264, 0.0048, 0.0046, 0.0747, 0.1356, 0.1548, 0.0632, 0.1025, 0.0454, 0.0711, 0.0216, 0.0617, 0.0513, 0.0527, 0.1083, 0.0234, 0.0712, 0.0732, 0.0035, 0.1081, 0.1021, 0.0316, 0.0685, 0.205, 0.0958, 0.1493, 0.0421, 0.1739, 0.2205, 0.0603, 0.0715, 0.128, 0.0916, 0.0761, 0.0181, 0.0804, 0.0221, 0.1261, 0.0579, 0.1947, 0.0353, 0.0832, 0.0526, 0.029, 0.08, 0.1511, 0.0949, 0.0902, 0.1262, 0.1016, 0.0686, 0.0173, 0.1352, 0.0761, 0.0451, 0.0031, 0.079, 0.0145, 0.137, 0.0928, 0.1109, 0.0756, 0.1244, 0.0719, 0.0237, 0.0703, 0.0289, 0.0263, 0.0955, 0.1562, 0.0417, 0.0565, 0.0955, 0.0663, 0.0448, 0.0376, 0.1596, 0.0686, 0.0538, 0.1139, 0.032, 0.0684, 0.1089, 0.0089, 0.0283, 0.0959, 0.0899, 0.5682, 0.1212, 0.1531, 0.1502, 0.2093, 0.1086, 0.0465, 0.0692, 0.0738, 0.1171, 0.0586, 0.0655, 0.0764, 0.1458, 0.0745, 0.0778, 0.2449, 0.0392, 0.0905, 0.0898, 0.0215, 0.081, 0.179, 0.063, 0.0753, 0.0742, 0.1198, 0.0857, 0.1177, 0.0779, 0.1276, 0.0371, 0.0034, 0.09, 0.0002, 0.1033, 0.1474, 0.169, 0.0603, 0.1101, 0.2284, 0.0839, 0.2114, 0.0787, 0.072, 0.0795, 0.2093, 0.0904, 0.0787, 0.0026, 0.1168, 0.1798, 0.0103, 0.0582, 0.1116, 0.024, 0.1207, 0.0371, 0.1073, 0.0928, 0.1418, 0.0525, 0.013, 0.0426, 0.0382, 0.0538, 0.2529, 0.0625, 0.0272, 0.021, 0.1789, 0.113, 0.013, 0.0414, 0.1272, 0.1371, 0.0001, 0.1038, 0.0871, 0.1584, 0.0075, 0.0699, 0.175, 0.1106, 0.0235, 0.0053, 0.0628, 0.0231, 0.1591, 0.0723, 0.0091, 0.1002, 0.0728, 0.0363, 0.1099, 0.0983, 0.046, 0.0619, 0.0673, 0.1238, 0.0008, 0.1245, 0.0924, 0.0826, 0.0168, 0.0545, 0.1681, 0.079, 0.1365, 0.0233, 0.3473, 0.2249, 0.0091, 0.1056, 0.0023, 0.048, 0.0602, 0.0232, 0.0635, 0.0501, 0.0002, 0.1376, 0.0773, 0.0797, 0.002, 0.0009, 0.0548, 0.0215, 0.0346, 0.0759, 0.0011, 0.1019, 0.0201, 0.0366, 0.0666, 0.1739, 0.2468, 0.3112, 0.0664, 0.0626, 0.1559, 0.1336, 0.0008, 0.2518, 0.0805, 0.0436, 0.0026, 0.0329, 0.1734, 0.219, 0.0174, 0.0108, 0.0642, 0.3439, 0.0413, 0.0548, 0.2634, 0.0094, 0.0003, 0.1246, 0.0541, 0.0007, 0.0116, 0.1389, 0.186, 0.0054, 0.0487, 0.1118, 0.2148, 0.1449, 0.0217, 0.0398, 0.0794, 0.1748, 0.0357, 0.0814, 0.1133, 0.1282, 0.1704, 0.0096, 0.0833, 0.0, 0.2083, 0.0133, 0.0262, 0.0001, 0.0894, 0.0636, 0.0149, 0.1181, 0.1218, 0.1165, 0.0208, 0.0548, 0.0777, 0.213, 0.0026, 0.0822, 0.1168, 0.0489, 0.4327, 0.0857, 0.0255, 0.1236, 0.0859, 0.0367, 0.1435, 0.1903, 0.0721, 0.0008, 0.004, 0.0516, 0.0582, 0.1967, 0.0136, 0.0665, 0.0304, 0.0011, 0.1421, 0.0609, 0.0783, 0.0552, 0.0926, 0.0847, 0.0156, 0.1375, 0.038, 0.0735, 0.0286, 0.0571, 0.0096, 0.0781, 0.2364, 0.0523, 0.0516, 0.0823, 0.021, 0.0992, 0.0946, 0.0494, 0.0002, 0.0363, 0.1596, 0.0568, 0.0868, 0.1509, 0.0731, 0.119, 0.0618, 0.1242, 0.0362, 0.0378, 0.1036, 0.0146, 0.011, 0.0734, 0.0616, 0.1357, 0.0623, 0.0047, 0.1104, 0.0273, 0.0508, 0.0012, 0.0436, 0.0544, 0.0006, 0.0609, 0.1905, 0.1026, 0.0395, 0.0387, 0.0782, 0.1754, 0.2166, 0.0739, 0.0583, 0.0452, 0.1098, 0.1039, 0.0391, 0.0422, 0.051, 0.0008, 0.0645, 0.135, 0.0585, 0.0014, 0.0008, 0.0001, 0.0503, 0.0004, 0.0922, 0.0714, 0.057, 0.0829, 0.0177, 0.0555, 0.0149, 0.0868, 0.1332, 0.0843, 0.0015, 0.0378, 0.0237, 0.2727, 0.0312, 0.1287, 0.1136, 0.0316, 0.0458, 0.0431, 0.1434, 0.1185, 0.0529, 0.115, 0.004, 0.0532, 0.0738, 0.0284, 0.0747, 0.0756, 0.0129, 0.1779, 0.0317, 0.0056, 0.2849, 0.0061, 0.4081, 0.0628, 0.1745, 0.0056, 0.0646, 0.0417, 0.0394, 0.0302, 0.0734, 0.1232, 0.0353, 0.1219, 0.1141, 0.0367, 0.0233, 0.1289, 0.065, 0.0464, 0.0969, 0.0505, 0.1592, 0.0003, 0.0064, 0.0506, 0.0381, 0.1159, 0.0542, 0.0926, 0.0611, 0.0686, 0.0511, 0.052, 0.0283, 0.1237, 0.0329, 0.0722, 0.0725, 0.1423, 0.0587, 0.1546, 0.0823, 0.0745, 0.0002, 0.1345, 0.0843, 0.0751, 0.0839, 0.114, 0.0359, 0.043, 0.0561, 0.0004, 0.0308, 0.1242, 0.0091, 0.2087, 0.0291, 0.0708, 0.4382, 0.0744, 0.0562, 0.0006, 0.0006, 0.0588, 0.1014, 0.0267, 0.0001, 0.154, 0.0916, 0.0005, 0.0717, 0.0096, 0.0015, 0.1314, 0.1915, 0.0098, 0.0023, 0.2334, 0.0845, 0.0677, 0.0858, 0.0175, 0.0282, 0.0296, 0.0899, 0.1566, 0.0481, 0.1371, 0.0648, 0.1541, 0.0122, 0.0261, 0.1041, 0.0002, 0.0476, 0.1266, 0.0474, 0.0523, 0.1187, 0.0198, 0.12, 0.1945, 0.065, 0.0497, 0.1174, 0.0095, 0.0886, 0.0399, 0.0002, 0.0493, 0.0009, 0.1156, 0.0034, 0.0549, 0.012, 0.0557, 0.0591, 0.2409, 0.0775, 0.0017, 0.1064, 0.0269, 0.0362, 0.1691, 0.0515, 0.0345, 0.065, 0.0975, 0.0682, 0.041, 0.0473, 0.0001, 0.0523, 0.0826, 0.0909, 0.0699, 0.0486, 0.0712, 0.0727, 0.1848, 0.0277, 0.0328, 0.0014, 0.0049, 0.1099, 0.0824, 0.0827, 0.0476, 0.1026, 0.0163, 0.0583, 0.0448, 0.129, 0.0003, 0.0649, 0.0338, 0.0431, 0.01, 0.0982, 0.1715, 0.0012, 0.0011, 0.0461, 0.1375, 0.0599, 0.0, 0.1085, 0.1487, 0.0613, 0.0, 0.0538, 0.0852, 0.0003, 0.0269, 0.0856, 0.0869, 0.0204, 0.0298, 0.0702, 0.0094, 0.0599, 0.0677, 0.1464, 0.1408, 0.0324, 0.0516, 0.1527, 0.0004, 0.0245, 0.0758, 0.0049, 0.0468, 0.0753, 0.0634, 0.0101, 0.0532, 0.0163, 0.0325, 0.0441, 0.5337, 0.092, 0.0698, 0.064, 0.0461, 0.1772, 0.0965, 0.0166, 0.0583, 0.0885, 0.0001, 0.1027, 0.0743, 0.0959, 0.063, 0.0335, 0.03, 0.011, 0.0369, 0.007, 0.0492, 0.0851, 0.0879, 0.0766, 0.031, 0.1812, 0.0593, 0.0007, 0.0105, 0.0442, 0.0454, 0.1235, 0.1556, 0.0005, 0.1278, 0.0972, 0.0686, 0.086, 0.102, 0.0585, 0.0329, 0.0801, 0.0472, 0.0005, 0.1417, 0.0485, 0.0924, 0.0883, 0.0705, 0.2663, 0.0651, 0.0884, 0.1254, 0.1725, 0.0875, 0.0006, 0.0452, 0.0027, 0.0932, 0.1003, 0.0715, 0.031, 0.0712, 0.0864, 0.0011, 0.0511, 0.0842, 0.0144, 0.0545, 0.0525, 0.2162, 0.0006, 0.0446, 0.1532, 0.0688, 0.1152, 0.1305, 0.0513, 0.0608, 0.036, 0.0854, 0.2038, 0.3377, 0.034, 0.0507, 0.09, 0.0005, 0.0493, 0.0327, 0.029, 0.0847, 0.0798, 0.0223, 0.0127, 0.0471, 0.0217, 0.0012, 0.062, 0.033, 0.0613, 0.0468, 0.0991, 0.0011, 0.0002, 0.0005, 0.0528, 0.1253, 0.0259, 0.0576, 0.029, 0.082, 0.0935, 0.0562, 0.0001, 0.0757, 0.0256, 0.0005, 0.0467, 0.1784, 0.0302, 0.0657, 0.0313, 0.0987, 0.1056, 0.0878, 0.0685, 0.0, 0.0125, 0.0479, 0.1106, 0.0274, 0.1397, 0.1489, 0.0669]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loss_hist_df.at[method_option, r_option] = str(result['loss_hist'])\n",
      "/tmp/ipykernel_93042/2001529989.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.156, 0.113, 0.096, 0.087, 0.081, 0.078, 0.075, 0.073, 0.069, 0.067]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loss_per_epoch_df.at[method_option, r_option] = str(result['loss_per_epoch'])\n",
      "/tmp/ipykernel_93042/2001529989.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[67.19442685243826, 68.1760607979734, 69.49018366054464, 69.09436352121597, 70.02849905003167, 71.42178594046865, 73.63837872070931, 72.46675110829639, 72.81507283090563, 72.29259024699176]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  val_acc_per_epoch_df.at[method_option, r_option] = str(result['val_acc_per_epoch'])\n",
      "/tmp/ipykernel_93042/2001529989.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.7184, 0.2349, 0.2304, 0.1273, 0.3047, 0.2429, 0.1623, 0.1635, 0.1665, 0.2757, 0.2289, 0.1202, 0.1231, 0.173, 0.1995, 0.1793, 0.1749, 0.2405, 0.1754, 0.1077, 0.1378, 0.1963, 0.1172, 0.2846, 0.1549, 0.0477, 0.1985, 0.1338, 0.1216, 0.1382, 0.0884, 0.1636, 0.1516, 0.4913, 0.1419, 0.1271, 0.0694, 0.1633, 0.1507, 0.2806, 0.1325, 0.25, 0.121, 0.1798, 0.1921, 0.3862, 0.0734, 0.2331, 0.3505, 0.0908, 0.0956, 0.1178, 0.0551, 0.1236, 0.1011, 0.2966, 0.1675, 0.1587, 0.2089, 0.0745, 0.1223, 0.186, 0.0083, 0.1323, 0.1153, 0.0914, 0.1549, 0.1507, 0.1148, 0.31, 0.1756, 0.0437, 0.1847, 0.053, 0.1257, 0.1095, 0.0749, 0.1315, 0.0348, 0.1333, 0.1187, 0.0693, 0.0717, 0.2206, 0.1539, 0.0453, 0.1456, 0.2141, 0.0959, 0.103, 0.0908, 0.2544, 0.0138, 0.1267, 0.138, 0.039, 0.1559, 0.0441, 0.0985, 0.0713, 0.0654, 0.1049, 0.132, 0.1556, 0.1295, 0.28, 0.1848, 0.0797, 0.2102, 0.1626, 0.1024, 0.091, 0.0433, 0.0893, 0.0292, 0.0866, 0.1518, 0.1089, 0.2151, 0.0261, 0.0862, 0.1619, 0.0535, 0.0523, 0.1033, 0.0857, 0.1648, 0.1955, 0.107, 0.1312, 0.0584, 0.1073, 0.0308, 0.1624, 0.1505, 0.2484, 0.1296, 0.1125, 0.1288, 0.143, 0.2037, 0.0886, 0.1996, 0.1357, 0.0295, 0.1016, 0.126, 0.0434, 0.097, 0.0512, 0.1588, 0.15, 0.1765, 0.2675, 0.3501, 0.1522, 0.04, 0.075, 0.1116, 0.1247, 0.0376, 0.104, 0.2451, 0.0358, 0.1038, 0.1284, 0.0583, 0.1479, 0.1952, 0.1185, 0.3269, 0.0578, 0.0377, 0.1499, 0.0939, 0.1716, 0.0766, 0.2426, 0.1835, 0.0559, 0.1029, 0.1579, 0.1972, 0.007, 0.0888, 0.0407, 0.175, 0.0705, 0.0885, 0.0671, 0.1174, 0.0942, 0.0575, 0.1175, 0.126, 0.0257, 0.0348, 0.005, 0.0548, 0.0602, 0.0701, 0.1968, 0.1353, 0.0543, 0.0863, 0.1604, 0.0844, 0.0144, 0.054, 0.0814, 0.261, 0.1381, 0.0637, 0.0067, 0.0751, 0.1732, 0.076, 0.0296, 0.1126, 0.2282, 0.1, 0.0209, 0.0049, 0.006, 0.0088, 0.1932, 0.1114, 0.1755, 0.1983, 0.1184, 0.0446, 0.0771, 0.0433, 0.0599, 0.0595, 0.0072, 0.1215, 0.0328, 0.055, 0.0571, 0.0149, 0.0668, 0.1141, 0.0391, 0.078, 0.2077, 0.1067, 0.1904, 0.0423, 0.1312, 0.0405, 0.0545, 0.0969, 0.0382, 0.0937, 0.3108, 0.0119, 0.0738, 0.0138, 0.2144, 0.029, 0.1999, 0.0798, 0.0309, 0.1705, 0.0448, 0.0877, 0.1929, 0.1043, 0.0981, 0.1143, 0.1214, 0.0596, 0.0223, 0.1228, 0.0767, 0.0348, 0.0309, 0.081, 0.0412, 0.1719, 0.0983, 0.1015, 0.0668, 0.3503, 0.1153, 0.0206, 0.0809, 0.0343, 0.0557, 0.0986, 0.0924, 0.0443, 0.0739, 0.043, 0.0498, 0.0498, 0.0606, 0.0953, 0.0379, 0.0495, 0.129, 0.0289, 0.0854, 0.0969, 0.0237, 0.0226, 0.0734, 0.0699, 0.1137, 0.0946, 0.1439, 0.0994, 0.2074, 0.0728, 0.0342, 0.0708, 0.0667, 0.1116, 0.0662, 0.0891, 0.0813, 0.1846, 0.1223, 0.2246, 0.2293, 0.0811, 0.1119, 0.0737, 0.0366, 0.0935, 0.147, 0.069, 0.063, 0.0543, 0.1302, 0.0592, 0.0302, 0.1448, 0.1382, 0.0279, 0.0069, 0.0698, 0.0006, 0.1106, 0.0755, 0.2288, 0.0493, 0.0863, 0.2042, 0.0928, 0.5666, 0.0881, 0.073, 0.0571, 0.1547, 0.1136, 0.092, 0.0017, 0.1235, 0.1352, 0.0064, 0.0854, 0.1034, 0.0209, 0.1262, 0.1109, 0.1341, 0.1097, 0.1733, 0.0432, 0.0257, 0.0479, 0.024, 0.048, 0.1084, 0.0415, 0.0309, 0.0036, 0.1852, 0.0981, 0.0254, 0.0392, 0.0886, 0.0781, 0.0001, 0.0753, 0.0885, 0.1433, 0.0015, 0.0718, 0.151, 0.092, 0.0299, 0.0391, 0.0542, 0.0134, 0.0705, 0.1121, 0.0035, 0.0882, 0.0398, 0.1457, 0.1092, 0.0814, 0.0187, 0.0581, 0.0738, 0.1383, 0.0039, 0.3883, 0.102, 0.0537, 0.0217, 0.0482, 0.1018, 0.066, 0.1349, 0.0115, 0.1579, 0.0728, 0.0116, 0.1177, 0.0037, 0.0759, 0.0403, 0.0197, 0.044, 0.0541, 0.0003, 0.1031, 0.0366, 0.1299, 0.0833, 0.0119, 0.068, 0.0241, 0.0048, 0.0773, 0.0015, 0.1686, 0.0589, 0.0398, 0.0808, 0.1951, 0.2738, 0.0797, 0.1006, 0.0654, 0.1523, 0.1497, 0.0853, 0.2325, 0.0716, 0.0172, 0.0061, 0.0076, 0.1696, 0.1397, 0.0141, 0.0622, 0.0626, 0.113, 0.0446, 0.1294, 0.2456, 0.0151, 0.0001, 0.0924, 0.0596, 0.0378, 0.0078, 0.1093, 0.2202, 0.0293, 0.1081, 0.1067, 0.2909, 0.1434, 0.0102, 0.0306, 0.0834, 0.1231, 0.0047, 0.0818, 0.1215, 0.1181, 0.0662, 0.0051, 0.0921, 0.0001, 0.076, 0.3751, 0.0427, 0.0, 0.0014, 0.1556, 0.0005, 0.0957, 0.1232, 0.1043, 0.0311, 0.0523, 0.0644, 0.2249, 0.0086, 0.0303, 0.1104, 0.0627, 0.1361, 0.0358, 0.0014, 0.1447, 0.1212, 0.0351, 0.131, 0.1087, 0.0478, 0.0099, 0.0674, 0.1869, 0.0452, 0.1885, 0.0327, 0.059, 0.0187, 0.0013, 0.1121, 0.0439, 0.0884, 0.0698, 0.0996, 0.1104, 0.0234, 0.1232, 0.0329, 0.0819, 0.0605, 0.0752, 0.0637, 0.0944, 0.1807, 0.0631, 0.0538, 0.1937, 0.119, 0.118, 0.0713, 0.0547, 0.0, 0.0502, 0.1397, 0.0339, 0.0761, 0.1496, 0.0602, 0.1083, 0.0334, 0.064, 0.0424, 0.0198, 0.1113, 0.0091, 0.0107, 0.1079, 0.06, 0.1263, 0.0493, 0.0021, 0.0787, 0.016, 0.0446, 0.0013, 0.0005, 0.0504, 0.0034, 0.0469, 0.1182, 0.0788, 0.0026, 0.1335, 0.0767, 0.1991, 0.1479, 0.0585, 0.0837, 0.0042, 0.138, 0.104, 0.0772, 0.0392, 0.0509, 0.0011, 0.0359, 0.1307, 0.0519, 0.0008, 0.0021, 0.0001, 0.0854, 0.0001, 0.0999, 0.0735, 0.0191, 0.0634, 0.011, 0.0516, 0.0117, 0.0883, 0.0276, 0.0703, 0.0045, 0.0379, 0.0015, 0.1574, 0.0245, 0.1422, 0.1123, 0.0276, 0.0471, 0.0333, 0.1411, 0.1216, 0.0668, 0.1094, 0.0058, 0.0599, 0.0583, 0.0388, 0.0579, 0.0801, 0.18, 0.1828, 0.0389, 0.0059, 0.2887, 0.0009, 0.1106, 0.1345, 0.2306, 0.0006, 0.0154, 0.0709, 0.0509, 0.0188, 0.0932, 0.0652, 0.0141, 0.1178, 0.1254, 0.0367, 0.0196, 0.128, 0.0526, 0.0425, 0.099, 0.1546, 0.1777, 0.0002, 0.0005, 0.0376, 0.035, 0.0876, 0.0325, 0.111, 0.0627, 0.0806, 0.0511, 0.0558, 0.0169, 0.1486, 0.0093, 0.0697, 0.0762, 0.1252, 0.0713, 0.142, 0.079, 0.0728, 0.0001, 0.0313, 0.0626, 0.163, 0.0882, 0.1296, 0.0387, 0.0486, 0.0429, 0.0001, 0.0274, 0.1154, 0.005, 0.1751, 0.0386, 0.0851, 0.0936, 0.0826, 0.0773, 0.0007, 0.0007, 0.0462, 0.0721, 0.0236, 0.0001, 0.1587, 0.0885, 0.0014, 0.0913, 0.006, 0.0025, 0.0665, 0.2141, 0.0066, 0.005, 0.0563, 0.0406, 0.1229, 0.0815, 0.0287, 0.0326, 0.0778, 0.0931, 0.1448, 0.0623, 0.1335, 0.079, 0.144, 0.0283, 0.054, 0.107, 0.0001, 0.1363, 0.1699, 0.0444, 0.0254, 0.1032, 0.0144, 0.0971, 0.1353, 0.063, 0.037, 0.1937, 0.021, 0.0626, 0.0374, 0.0001, 0.0522, 0.0324, 0.0862, 0.0004, 0.0529, 0.1455, 0.0703, 0.0634, 0.145, 0.034, 0.0045, 0.1098, 0.0361, 0.0572, 0.1515, 0.0463, 0.0376, 0.0555, 0.0478, 0.0597, 0.0504, 0.0431, 0.0043, 0.048, 0.0983, 0.09, 0.0853, 0.0545, 0.0662, 0.0717]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loss_hist_df.at[method_option, r_option] = str(result['loss_hist'])\n",
      "/tmp/ipykernel_93042/2001529989.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.156, 0.117, 0.099, 0.088, 0.082, 0.077, 0.073, 0.072]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loss_per_epoch_df.at[method_option, r_option] = str(result['loss_per_epoch'])\n",
      "/tmp/ipykernel_93042/2001529989.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[66.2444585180494, 67.30525649145028, 70.55098163394553, 70.91513616212794, 73.55921469284357, 73.22672577580748, 72.89423685877138, 73.38505383153895]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  val_acc_per_epoch_df.at[method_option, r_option] = str(result['val_acc_per_epoch'])\n",
      "/tmp/ipykernel_93042/2001529989.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.7188, 0.2298, 0.1928, 0.1648, 0.3075, 0.2079, 0.1595, 0.1739, 0.1468, 0.3002, 0.2216, 0.1177, 0.1066, 0.1541, 0.2071, 0.1492, 0.1615, 0.2458, 0.1961, 0.1023, 0.1566, 0.193, 0.1165, 0.251, 0.1609, 0.0508, 0.1357, 0.1544, 0.0858, 0.1294, 0.0866, 0.1143, 0.1109, 0.4781, 0.1239, 0.1374, 0.066, 0.1694, 0.1598, 0.3461, 0.1282, 0.2496, 0.1495, 0.1685, 0.1386, 0.2404, 0.1056, 0.1645, 0.1974, 0.0883, 0.1332, 0.1282, 0.0699, 0.2352, 0.0803, 0.2313, 0.075, 0.1669, 0.1718, 0.1136, 0.0997, 0.1602, 0.0137, 0.1148, 0.114, 0.089, 0.2312, 0.1624, 0.0627, 0.0443, 0.2074, 0.0375, 0.1679, 0.0844, 0.1866, 0.1078, 0.067, 0.1118, 0.0388, 0.0816, 0.0861, 0.0511, 0.2382, 0.201, 0.1351, 0.0464, 0.1238, 0.2067, 0.0538, 0.1224, 0.1056, 0.2459, 0.022, 0.062, 0.163, 0.0263, 0.1527, 0.1002, 0.074, 0.0856, 0.1513, 0.1492, 0.1282, 0.1547, 0.1475, 0.308, 0.1647, 0.0666, 0.3279, 0.1286, 0.0998, 0.0767, 0.0257, 0.1318, 0.0427, 0.1449, 0.2294, 0.0769, 0.1693, 0.0296, 0.0913, 0.0535, 0.036, 0.0523, 0.1791, 0.0658, 0.1515, 0.1909, 0.1066, 0.1106, 0.06, 0.1125, 0.0311, 0.2284, 0.179, 0.2209, 0.0888, 0.1929, 0.0776, 0.1194, 0.1577, 0.1222, 0.173, 0.1076, 0.0524, 0.1014, 0.1188, 0.0369, 0.0947, 0.05, 0.2063, 0.146, 0.1867, 0.2768, 0.4171, 0.0887, 0.0374, 0.0999, 0.0024, 0.1246, 0.2019, 0.1015, 0.1286, 0.0085, 0.0528, 0.1459, 0.0666, 0.1169, 0.1687, 0.1853, 0.3056, 0.037, 0.0465, 0.1491, 0.1572, 0.1717, 0.0965, 0.2276, 0.1097, 0.057, 0.0483, 0.211, 0.1754, 0.0056, 0.073, 0.0535, 0.1265, 0.0773, 0.0779, 0.0598, 0.1599, 0.0715, 0.0353, 0.115, 0.2145, 0.0294, 0.0911, 0.0441, 0.0724, 0.1084, 0.0834, 0.1346, 0.1015, 0.112, 0.0649, 0.1195, 0.1239, 0.0218, 0.0596, 0.0941, 0.2265, 0.1752, 0.0716, 0.1017, 0.0793, 0.0853, 0.0859, 0.0176, 0.09, 0.245, 0.1525, 0.0198, 0.0537, 0.0093, 0.0099, 0.0908, 0.0777, 0.1422, 0.13, 0.1414, 0.051, 0.1027, 0.086, 0.0502, 0.0682, 0.0293, 0.1058, 0.0271, 0.0532, 0.0709, 0.0108, 0.204, 0.149, 0.0363, 0.0573, 0.2003, 0.1244, 0.2083, 0.0464, 0.0863, 0.0591, 0.071, 0.0598, 0.0542, 0.0887, 0.0825, 0.0159, 0.0582, 0.0007, 0.141, 0.0286, 0.1437, 0.0633, 0.0628, 0.1257, 0.0126, 0.0442, 0.1322, 0.1088, 0.1041, 0.1151, 0.1953, 0.0324, 0.0513, 0.1732, 0.0663, 0.0335, 0.053, 0.0742, 0.0233, 0.1409, 0.1025, 0.1221, 0.1238, 0.2081, 0.0565, 0.0187, 0.0817, 0.0356, 0.0802, 0.0998, 0.1092, 0.0793, 0.0481, 0.0282, 0.0642, 0.0475, 0.0275, 0.2509, 0.0708, 0.0797, 0.2998, 0.0292, 0.0684, 0.1168, 0.0257, 0.0235, 0.08, 0.0968, 0.0966, 0.0787, 0.1793, 0.1744, 0.2367, 0.0433, 0.0618, 0.0973, 0.0797, 0.1263, 0.059, 0.0896, 0.0855, 0.1831, 0.0455, 0.1573, 0.1839, 0.0126, 0.1329, 0.0852, 0.0598, 0.0796, 0.2026, 0.0643, 0.0633, 0.0362, 0.1259, 0.028, 0.0212, 0.1002, 0.1587, 0.0335, 0.0026, 0.0535, 0.002, 0.1388, 0.101, 0.1384, 0.0531, 0.1034, 0.1913, 0.0663, 0.1812, 0.1667, 0.1125, 0.3109, 0.1593, 0.0907, 0.0772, 0.0018, 0.1024, 0.1685, 0.0103, 0.0896, 0.1234, 0.0222, 0.1403, 0.0193, 0.3913, 0.1242, 0.1457, 0.0752, 0.0132, 0.0862, 0.0307, 0.0461, 0.1337, 0.0424, 0.0336, 0.0258, 0.1505, 0.1328, 0.0297, 0.0421, 0.2521, 0.1005, 0.0002, 0.1204, 0.0897, 0.1494, 0.0053, 0.0693, 0.0735, 0.1129, 0.0322, 0.0934, 0.0078, 0.0337, 0.0497, 0.1075, 0.0039, 0.0829, 0.2264, 0.0312, 0.1143, 0.1068, 0.0289, 0.0589, 0.0745, 0.129, 0.0055, 0.3375, 0.0883, 0.1142, 0.0175, 0.0461, 0.1504, 0.0774, 0.1445, 0.0171, 0.1457, 0.0684, 0.0275, 0.1056, 0.0056, 0.081, 0.0594, 0.0233, 0.0398, 0.0487, 0.0004, 0.2343, 0.0265, 0.227, 0.0479, 0.0034, 0.0567, 0.0194, 0.1343, 0.0969, 0.0068, 0.0971, 0.0245, 0.05, 0.0777, 0.2118, 0.2872, 0.1546, 0.0533, 0.0673, 0.1502, 0.1666, 0.0058, 0.174, 0.0606, 0.0684, 0.0121, 0.0204, 0.1902, 0.1582, 0.0087, 0.0624, 0.0629, 0.1122, 0.0488, 0.0706, 0.2085, 0.0088, 0.0013, 0.1285, 0.065, 0.1031, 0.0087, 0.0926, 0.2255, 0.1105, 0.13, 0.0874, 0.1301, 0.133, 0.0259, 0.0301, 0.0815, 0.1273, 0.0216, 0.0818, 0.1062, 0.1318, 0.0722, 0.0332, 0.0961, 0.0001, 0.2328, 0.0047, 0.0369, 0.0001, 0.0098, 0.0692, 0.0043, 0.0716, 0.1209, 0.1834, 0.028, 0.0569, 0.0765, 0.2144, 0.0139, 0.054, 0.1231, 0.0604, 0.3667, 0.0323, 0.0031, 0.1346, 0.1095, 0.0427, 0.1056, 0.1456, 0.0802, 0.0081, 0.0017, 0.0558, 0.0519, 0.1627, 0.0471, 0.0473, 0.0218, 0.0021, 0.112, 0.2791, 0.0793, 0.0569, 0.1131, 0.1015, 0.0072, 0.1024, 0.053, 0.0051, 0.0613, 0.0691, 0.025, 0.0894, 0.1207, 0.0656, 0.1893, 0.063, 0.1111, 0.1183, 0.0811, 0.0417, 0.0001, 0.0483, 0.1643, 0.0586, 0.151, 0.1571, 0.0646, 0.1068, 0.0105, 0.0657, 0.024, 0.0369, 0.105, 0.0176, 0.018, 0.0907, 0.1085, 0.1285, 0.0585, 0.0007, 0.0984, 0.0047, 0.0555, 0.0037, 0.0434, 0.0493, 0.196, 0.0391, 0.1704, 0.0859, 0.0267, 0.0655, 0.0786, 0.1888, 0.1222, 0.1068, 0.1389, 0.1014, 0.107, 0.1042, 0.1027, 0.0416, 0.0544, 0.006, 0.0126, 0.1471, 0.0586, 0.0009, 0.0017, 0.0005, 0.0476, 0.0024, 0.0945, 0.0733, 0.019, 0.0685, 0.016, 0.0531, 0.0218, 0.0669, 0.1644, 0.0705, 0.0033, 0.0426, 0.0036, 0.0891, 0.0212, 0.1386, 0.1057, 0.0231, 0.0434, 0.0085, 0.1274, 0.1364, 0.05, 0.1325, 0.0183, 0.068, 0.0303, 0.0278, 0.046, 0.0855, 0.1365, 0.1654, 0.0131, 0.0035, 0.2467, 0.0057, 0.1629, 0.0631, 0.2903, 0.0016, 0.0344, 0.0466, 0.0406, 0.0269, 0.0559, 0.0572, 0.1384, 0.0909, 0.1249, 0.0332, 0.0253, 0.1398, 0.039, 0.0365, 0.0831, 0.0969, 0.1783, 0.0004, 0.0017, 0.0428, 0.0436, 0.0896, 0.0333, 0.1189, 0.0605, 0.0661, 0.0489, 0.0477, 0.02, 0.1739, 0.0022, 0.078, 0.0896, 0.1296, 0.0666, 0.1645, 0.0704, 0.0753, 0.0276, 0.0204, 0.056, 0.0703, 0.0742, 0.1468, 0.0386, 0.0462, 0.0542, 0.0002, 0.0175, 0.111, 0.0008, 0.195, 0.0767, 0.0749, 0.1059, 0.0676, 0.0552, 0.0022, 0.0004, 0.0532, 0.1641, 0.023, 0.001, 0.1585, 0.0913, 0.0018, 0.1061, 0.0123, 0.0013, 0.0549, 0.2276, 0.006, 0.0126, 0.005, 0.0301, 0.0236, 0.0837, 0.0299, 0.0122, 0.0684, 0.1024, 0.1555, 0.0485, 0.222, 0.0885, 0.1568, 0.0069, 0.0168, 0.1164, 0.0005, 0.0739, 0.1345, 0.0515, 0.0511, 0.1102, 0.0479, 0.1077, 0.1317, 0.0943, 0.0591, 0.1079, 0.0467, 0.0065, 0.0425, 0.0001, 0.051, 0.0005, 0.1191, 0.0008, 0.1407, 0.0097, 0.0891, 0.0675, 0.1418, 0.0837, 0.0025, 0.0892, 0.11, 0.0484, 0.1603, 0.0452, 0.0407, 0.1868, 0.0918, 0.0596, 0.0397, 0.048, 0.0017, 0.0506, 0.0752, 0.0554, 0.0917, 0.0646, 0.058, 0.0732, 0.1498, 0.0695, 0.0202, 0.004, 0.0237, 0.1095, 0.0801, 0.0748, 0.0484, 0.1254, 0.0161, 0.0573, 0.0642, 0.1907, 0.0001, 0.0737, 0.036, 0.0825, 0.0053, 0.4678, 0.1959, 0.0176, 0.0018, 0.0481, 0.0265, 0.0768, 0.0001, 0.0807, 0.3363, 0.0241, 0.0003, 0.0369, 0.0446, 0.0003, 0.0432, 0.0849, 0.0898, 0.0204, 0.036, 0.0614, 0.034, 0.0049, 0.0685, 0.0095, 0.1375, 0.0798, 0.076, 0.1988, 0.0099, 0.0258, 0.0752, 0.0084, 0.0475, 0.0922, 0.0739, 0.0151, 0.0583, 0.022, 0.0336, 0.0461, 0.0853, 0.1082, 0.1137, 0.0558, 0.0603, 0.172, 0.065, 0.1749, 0.0686, 0.0609, 0.0011, 0.0802, 0.1308, 0.1681, 0.0462, 0.049, 0.0143, 0.0103, 0.0839, 0.0059, 0.049, 0.0598, 0.142, 0.1175, 0.0265, 0.1686, 0.0437, 0.0025, 0.0266, 0.0398, 0.0459, 0.0742, 0.1445, 0.001, 0.1445, 0.0945, 0.069, 0.0519, 0.0968, 0.0621, 0.0604, 0.0744, 0.0767, 0.0005, 0.1118, 0.1285, 0.0905, 0.0684, 0.054, 0.4329, 0.0615, 0.0678, 0.0893, 0.0529, 0.1379, 0.0004, 0.0425, 0.0013, 0.0886, 0.0863, 0.0621, 0.0422, 0.0487, 0.0658, 0.0277, 0.0586, 0.0867, 0.0009, 0.0451, 0.0412, 0.1608, 0.0005, 0.0472, 0.1172, 0.1448, 0.1264, 0.1647, 0.0672, 0.0236, 0.0042, 0.0811, 0.2292, 0.0766, 0.0188, 0.1265, 0.0988, 0.0006, 0.053, 0.0264, 0.0306, 0.1052, 0.0501, 0.0114, 0.0118, 0.0989, 0.0266, 0.0004, 0.0971, 0.0581, 0.0679, 0.1105, 0.0936, 0.001, 0.0006, 0.0002, 0.0625, 0.1532, 0.0615, 0.0424, 0.0256, 0.0808, 0.1239, 0.0565, 0.0001, 0.0844, 0.0003, 0.0004, 0.0712, 0.2029, 0.0207, 0.0742, 0.0257, 0.1098, 0.0887, 0.1292, 0.0863, 0.0003, 0.0102, 0.0502, 0.1392, 0.0542, 0.0559, 0.0572, 0.0662, 0.0888, 0.0009, 0.0313, 0.1166, 0.1485, 0.0397, 0.0496, 0.1016, 0.051, 0.0331, 0.036, 0.0825, 0.0471, 0.0677, 0.0461, 0.0, 0.0009, 0.0413, 0.0594, 0.0371, 0.0023, 0.0306, 0.0415, 0.0233, 0.056, 0.1113, 0.0643, 0.008, 0.1229, 0.0804, 0.0029, 0.0625, 0.0539, 0.0011, 0.1784, 0.0001, 0.0315, 0.065, 0.0006, 0.0691, 0.1291, 0.0394, 0.0475, 0.0285, 0.0237, 0.0025, 0.1372, 0.1395, 0.1411, 0.0349, 0.1537, 0.0608, 0.0466, 0.057, 0.0661, 0.0002, 0.086, 0.0436, 0.0778, 0.0353, 0.0568, 0.0408, 0.0033, 0.1173, 0.0902, 0.1749, 0.1557, 0.1386, 0.0911, 0.0155, 0.0001, 0.0001, 0.0754, 0.0697, 0.0034, 0.0019, 0.0316, 0.0893, 0.0738, 0.0002, 0.025, 0.0753, 0.0759, 0.0486, 0.0259, 0.0712, 0.0923, 0.0543, 0.0559, 0.1246, 0.0, 0.045, 0.0613, 0.0007, 0.0551, 0.0636, 0.065, 0.0004, 0.0333, 0.0831, 0.0611, 0.0428, 0.0598, 0.0469, 0.0377, 0.0945, 0.0072, 0.1011, 0.0239, 0.0538, 0.0007, 0.0046, 0.0591, 0.0982, 0.0089, 0.0875, 0.0471, 0.1203, 0.0749, 0.0358, 0.09, 0.1024, 0.0999, 0.0507, 0.0577, 0.0206, 0.0126, 0.0021, 0.1053, 0.0003, 0.0001, 0.0248, 0.1549, 0.0011, 0.0367, 0.0607, 0.2057, 0.0745, 0.1466, 0.0217, 0.1106, 0.043, 0.0223, 0.1295, 0.0939, 0.0967, 0.0382, 0.0655, 0.0143, 0.1437, 0.2369, 0.1929, 0.0805, 0.001, 0.2827, 0.0242, 0.0415, 0.0792, 0.0375, 0.0032, 0.0491, 0.1123, 0.0001, 0.0053, 0.0734, 0.1948, 0.0213, 0.0337, 0.0531, 0.0221, 0.113, 0.0658, 0.0195, 0.2844, 0.0318, 0.1044, 0.0944, 0.0992, 0.0995, 0.0431, 0.0438, 0.1069, 0.0774, 0.0551, 0.0628, 0.0099, 0.0262, 0.0186, 0.0237, 0.2271, 0.1635, 0.1212, 0.0784, 0.0626, 0.0005, 0.1319, 0.0928, 0.0404, 0.0704, 0.0008, 0.0666, 0.1332, 0.0916, 0.0324, 0.0652, 0.0678, 0.0343, 0.0599, 0.0527, 0.0656, 0.0008, 0.1589, 0.0995, 0.1432, 0.0723, 0.0533, 0.063, 0.0285, 0.0321, 0.1107, 0.0537, 0.0372, 0.0443, 0.0425, 0.0022, 0.0008, 0.0489, 0.0003, 0.0371, 0.1839, 0.0112, 0.0223, 0.0451, 0.1684, 0.0299, 0.0489, 0.1224, 0.0073, 0.0377, 0.0284, 0.0014, 0.2512, 0.0916, 0.0643, 0.0572, 0.0107, 0.0, 0.0204, 0.0004, 0.1468, 0.0323, 0.0476, 0.0003, 0.0215, 0.1308, 0.0062, 0.0075, 0.0808, 0.032, 0.0969, 0.0085, 0.0683, 0.0266, 0.0001, 0.0736, 0.0605, 0.0536, 0.0393, 0.0097, 0.1235, 0.0331, 0.0966, 0.1431, 0.0267, 0.0394, 0.1254, 0.0001, 0.1552, 0.0432, 0.0262, 0.0089, 0.1063, 0.0377, 0.0811, 0.0712, 0.0434, 0.0005, 0.0022, 0.0562, 0.0568, 0.0602, 0.0227, 0.0006, 0.0001, 0.0481, 0.0002, 0.0079, 0.0681, 0.0926, 0.0404, 0.0007, 0.0575, 0.0334, 0.0001, 0.0803, 0.0179, 0.0603, 0.0003, 0.0001, 0.0357, 0.0927, 0.0214, 0.0644, 0.2097, 0.0492, 0.036, 0.0704, 0.0544, 0.0866, 0.0172, 0.0001, 0.1679, 0.0001, 0.1119, 0.0835, 0.1012, 0.0211, 0.0332, 0.0873, 0.0189, 0.0678, 0.0001, 0.0552, 0.0024, 0.0489, 0.0549, 0.0565, 0.067, 0.0018, 0.128, 0.0365, 0.0939, 0.0622, 0.0265, 0.0546, 0.1729, 0.024, 0.0059, 0.1642, 0.0185, 0.0155, 0.076, 0.0666, 0.0566, 0.0001, 0.0003, 0.1123, 0.0394, 0.081, 0.0305, 0.0227, 0.1021, 0.034, 0.1119, 0.0346, 0.0673, 0.0405, 0.0159, 0.086, 0.0516, 0.0177, 0.0448, 0.067, 0.0009, 0.0363, 0.109, 0.1604, 0.0687, 0.0482, 0.0001, 0.0001, 0.0287, 0.0714, 0.0748, 0.1687, 0.0016, 0.0876, 0.0573, 0.0269, 0.0681, 0.0301, 0.0399, 0.0942, 0.0526, 0.0544, 0.1324, 0.0209, 0.0935, 0.0549, 0.0486, 0.0007, 0.0031, 0.0757, 0.0284, 0.0412, 0.0221, 0.0888, 0.0581, 0.0417, 0.0286, 0.0, 0.0002, 0.1135, 0.0, 0.117, 0.0021, 0.0001, 0.047, 0.0539, 0.0014, 0.0977, 0.1306, 0.0001, 0.1095, 0.017, 0.0643, 0.0033, 0.1012, 0.1506, 0.0162, 0.0542, 0.0467, 0.0002, 0.0479, 0.0081, 0.1638, 0.0854, 0.0297, 0.1144, 0.0643, 0.091, 0.0949, 0.0005, 0.0564, 0.0714, 0.0, 0.1525, 0.0005, 0.0564, 0.0001, 0.1682, 0.0516, 0.0526, 0.0358, 0.0189, 0.0376, 0.0409, 0.0329, 0.0521, 0.0455, 0.0691, 0.0335, 0.0091, 0.0087, 0.0828, 0.0002, 0.2173, 0.1587, 0.042, 0.0696, 0.0228, 0.0438, 0.1714, 0.1315, 0.0001, 0.0308, 0.0012, 0.0297, 0.0605, 0.0501, 0.0002, 0.1401, 0.0042, 0.0356, 0.0561]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loss_hist_df.at[method_option, r_option] = str(result['loss_hist'])\n",
      "/tmp/ipykernel_93042/2001529989.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.153, 0.115, 0.1, 0.09, 0.082, 0.079, 0.075, 0.073, 0.069, 0.066, 0.065, 0.063, 0.06, 0.057, 0.056]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loss_per_epoch_df.at[method_option, r_option] = str(result['loss_per_epoch'])\n",
      "/tmp/ipykernel_93042/2001529989.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[66.07029765674477, 66.2444585180494, 67.47941735275491, 69.07853071564281, 69.99683343888537, 71.35845471817606, 71.97593413552882, 73.76504116529449, 72.26092463584547, 74.11336288790373, 72.67257758074732, 73.84420519316024, 74.3350221659278, 74.50918302723242, 74.6991766941102]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  val_acc_per_epoch_df.at[method_option, r_option] = str(result['val_acc_per_epoch'])\n"
     ]
    }
   ],
   "source": [
    "test_accuracy_df = pd.DataFrame(np.zeros((19,4)), columns=r_option_list, index=method_option_list)\n",
    "val_accuracy_df = test_accuracy_df.copy()\n",
    "training_time_df = test_accuracy_df.copy()\n",
    "time_per_it_df = test_accuracy_df.copy()\n",
    "trainable_params_df = test_accuracy_df.copy()\n",
    "loss_hist_df = test_accuracy_df.copy()\n",
    "loss_per_epoch_df = test_accuracy_df.copy()\n",
    "val_acc_per_epoch_df = test_accuracy_df.copy()\n",
    "\n",
    "for result in result_list:\n",
    "    id = result['experiment_id']\n",
    "    \n",
    "    method_option, r_option = experiment_id_to_info(id)\n",
    "    test_accuracy_df.at[method_option, r_option] = result['test_accuracy']\n",
    "    val_accuracy_df.at[method_option, r_option] = result['val_accuracy']\n",
    "    training_time_df.at[method_option, r_option] = result['training_time']\n",
    "    time_per_it_df.at[method_option, r_option] = result['time_per_it']\n",
    "    trainable_params_df.at[method_option, r_option] = result['llm_trainable_params'] + result['qformer_trainable_params']\n",
    "    loss_hist_df.at[method_option, r_option] = str(result['loss_hist'])\n",
    "    loss_per_epoch_df.at[method_option, r_option] = str(result['loss_per_epoch'])\n",
    "    val_acc_per_epoch_df.at[method_option, r_option] = str(result['val_acc_per_epoch'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r = 1</th>\n",
       "      <th>r = 2</th>\n",
       "      <th>r = 4</th>\n",
       "      <th>r = 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>0.872583</td>\n",
       "      <td>0.852256</td>\n",
       "      <td>0.863659</td>\n",
       "      <td>0.865146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, FlanT5-XL)</th>\n",
       "      <td>0.862172</td>\n",
       "      <td>0.848785</td>\n",
       "      <td>0.849281</td>\n",
       "      <td>0.856718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>0.854239</td>\n",
       "      <td>0.864155</td>\n",
       "      <td>0.856222</td>\n",
       "      <td>0.856718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>0.844819</td>\n",
       "      <td>0.844819</td>\n",
       "      <td>0.844819</td>\n",
       "      <td>0.844819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, FlanT5-XL)</th>\n",
       "      <td>0.843332</td>\n",
       "      <td>0.843332</td>\n",
       "      <td>0.801686</td>\n",
       "      <td>0.843332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, FlanT5-XL)</th>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.809122</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, FlanT5-XL)</th>\n",
       "      <td>0.815568</td>\n",
       "      <td>0.815568</td>\n",
       "      <td>0.815568</td>\n",
       "      <td>0.815568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>0.872583</td>\n",
       "      <td>0.861180</td>\n",
       "      <td>0.870104</td>\n",
       "      <td>0.862667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, Vicuna-7B)</th>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.829945</td>\n",
       "      <td>0.829945</td>\n",
       "      <td>0.829945</td>\n",
       "      <td>0.829945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, Vicuna-7B)</th>\n",
       "      <td>0.823004</td>\n",
       "      <td>0.823004</td>\n",
       "      <td>0.823004</td>\n",
       "      <td>0.823004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, Vicuna-7B)</th>\n",
       "      <td>0.841349</td>\n",
       "      <td>0.841349</td>\n",
       "      <td>0.841349</td>\n",
       "      <td>0.841349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.810114</td>\n",
       "      <td>0.810114</td>\n",
       "      <td>0.810114</td>\n",
       "      <td>0.810114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "      <td>0.355478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (self-attn QVO, FlanT5-XL)</th>\n",
       "      <td>0.854239</td>\n",
       "      <td>0.854239</td>\n",
       "      <td>0.808131</td>\n",
       "      <td>0.854239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QVO, FlanT5-XL)</th>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.809122</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QV, FlanT5-XL)</th>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.809122</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.847298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           r = 1     r = 2     r = 4     r = 8\n",
       "LLM LoRA (ffn, FlanT5-XL)               0.872583  0.852256  0.863659  0.865146\n",
       "LLM LoRA (attn, FlanT5-XL)              0.862172  0.848785  0.849281  0.856718\n",
       "LLM LoRA (all, FlanT5-XL)               0.854239  0.864155  0.856222  0.856718\n",
       "Q-Former LoRA (ffn, FlanT5-XL)          0.844819  0.844819  0.844819  0.844819\n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    0.843332  0.843332  0.801686  0.843332\n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   0.847298  0.809122  0.847298  0.847298\n",
       "Q-Former LoRA (all, FlanT5-XL)          0.815568  0.815568  0.815568  0.815568\n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  0.872583  0.861180  0.870104  0.862667\n",
       "LLM LoRA (ffn, Vicuna-7B)               0.355478  0.355478  0.355478  0.355478\n",
       "LLM LoRA (attn, Vicuna-7B)              0.355478  0.355478  0.355478  0.355478\n",
       "LLM LoRA (all, Vicuna-7B)               0.355478  0.355478  0.355478  0.355478\n",
       "Q-Former LoRA (ffn, Vicuna-7B)          0.829945  0.829945  0.829945  0.829945\n",
       "Q-Former LoRA (self-attn, Vicuna-7B)    0.823004  0.823004  0.823004  0.823004\n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)   0.841349  0.841349  0.841349  0.841349\n",
       "Q-Former LoRA (all, Vicuna-7B)          0.810114  0.810114  0.810114  0.810114\n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)  0.355478  0.355478  0.355478  0.355478\n",
       "Q-Former (self-attn QVO, FlanT5-XL)     0.854239  0.854239  0.808131  0.854239\n",
       "Q-Former (cross-attn QVO, FlanT5-XL)    0.847298  0.809122  0.847298  0.847298\n",
       "Q-Former (cross-attn QV, FlanT5-XL)     0.847298  0.809122  0.847298  0.847298"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy_df.to_csv(f'{dataset}/test_accuracy.csv', index=True)\n",
    "test_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r = 1</th>\n",
       "      <th>r = 2</th>\n",
       "      <th>r = 4</th>\n",
       "      <th>r = 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>0.853600</td>\n",
       "      <td>0.826419</td>\n",
       "      <td>0.834526</td>\n",
       "      <td>0.834526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, FlanT5-XL)</th>\n",
       "      <td>0.829757</td>\n",
       "      <td>0.834049</td>\n",
       "      <td>0.833095</td>\n",
       "      <td>0.832141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>0.828326</td>\n",
       "      <td>0.847401</td>\n",
       "      <td>0.839771</td>\n",
       "      <td>0.829757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>0.824988</td>\n",
       "      <td>0.824988</td>\n",
       "      <td>0.824988</td>\n",
       "      <td>0.824988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, FlanT5-XL)</th>\n",
       "      <td>0.827849</td>\n",
       "      <td>0.827849</td>\n",
       "      <td>0.788269</td>\n",
       "      <td>0.827849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, FlanT5-XL)</th>\n",
       "      <td>0.824988</td>\n",
       "      <td>0.802575</td>\n",
       "      <td>0.824988</td>\n",
       "      <td>0.824988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, FlanT5-XL)</th>\n",
       "      <td>0.809251</td>\n",
       "      <td>0.809251</td>\n",
       "      <td>0.809251</td>\n",
       "      <td>0.809251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>0.850739</td>\n",
       "      <td>0.851216</td>\n",
       "      <td>0.856462</td>\n",
       "      <td>0.852170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, Vicuna-7B)</th>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.793515</td>\n",
       "      <td>0.793515</td>\n",
       "      <td>0.793515</td>\n",
       "      <td>0.793515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, Vicuna-7B)</th>\n",
       "      <td>0.809728</td>\n",
       "      <td>0.809728</td>\n",
       "      <td>0.809728</td>\n",
       "      <td>0.809728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, Vicuna-7B)</th>\n",
       "      <td>0.810682</td>\n",
       "      <td>0.810682</td>\n",
       "      <td>0.810682</td>\n",
       "      <td>0.810682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.788746</td>\n",
       "      <td>0.788746</td>\n",
       "      <td>0.788746</td>\n",
       "      <td>0.788746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>0.371006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           r = 1     r = 2     r = 4     r = 8\n",
       "LLM LoRA (ffn, FlanT5-XL)               0.853600  0.826419  0.834526  0.834526\n",
       "LLM LoRA (attn, FlanT5-XL)              0.829757  0.834049  0.833095  0.832141\n",
       "LLM LoRA (all, FlanT5-XL)               0.828326  0.847401  0.839771  0.829757\n",
       "Q-Former LoRA (ffn, FlanT5-XL)          0.824988  0.824988  0.824988  0.824988\n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    0.827849  0.827849  0.788269  0.827849\n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   0.824988  0.802575  0.824988  0.824988\n",
       "Q-Former LoRA (all, FlanT5-XL)          0.809251  0.809251  0.809251  0.809251\n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  0.850739  0.851216  0.856462  0.852170\n",
       "LLM LoRA (ffn, Vicuna-7B)               0.371006  0.371006  0.371006  0.371006\n",
       "LLM LoRA (attn, Vicuna-7B)              0.371006  0.371006  0.371006  0.371006\n",
       "LLM LoRA (all, Vicuna-7B)               0.371006  0.371006  0.371006  0.371006\n",
       "Q-Former LoRA (ffn, Vicuna-7B)          0.793515  0.793515  0.793515  0.793515\n",
       "Q-Former LoRA (self-attn, Vicuna-7B)    0.809728  0.809728  0.809728  0.809728\n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)   0.810682  0.810682  0.810682  0.810682\n",
       "Q-Former LoRA (all, Vicuna-7B)          0.788746  0.788746  0.788746  0.788746\n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)  0.371006  0.371006  0.371006  0.371006"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracy_df.to_csv(f'{dataset}/val_accuracy.csv', index=True)\n",
    "val_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r = 1</th>\n",
       "      <th>r = 2</th>\n",
       "      <th>r = 4</th>\n",
       "      <th>r = 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>3.658333</td>\n",
       "      <td>2.751944</td>\n",
       "      <td>3.770833</td>\n",
       "      <td>3.788611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, FlanT5-XL)</th>\n",
       "      <td>2.425278</td>\n",
       "      <td>2.679722</td>\n",
       "      <td>2.674722</td>\n",
       "      <td>2.923889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>3.126389</td>\n",
       "      <td>3.912778</td>\n",
       "      <td>2.881111</td>\n",
       "      <td>3.175556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>3.606667</td>\n",
       "      <td>3.638889</td>\n",
       "      <td>3.643056</td>\n",
       "      <td>3.634167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, FlanT5-XL)</th>\n",
       "      <td>2.804167</td>\n",
       "      <td>2.840833</td>\n",
       "      <td>2.611944</td>\n",
       "      <td>2.830278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, FlanT5-XL)</th>\n",
       "      <td>3.443611</td>\n",
       "      <td>3.438889</td>\n",
       "      <td>3.451389</td>\n",
       "      <td>3.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, FlanT5-XL)</th>\n",
       "      <td>1.813889</td>\n",
       "      <td>1.849444</td>\n",
       "      <td>1.848889</td>\n",
       "      <td>1.846667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>3.884722</td>\n",
       "      <td>3.280278</td>\n",
       "      <td>4.500278</td>\n",
       "      <td>3.882778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.968333</td>\n",
       "      <td>0.981667</td>\n",
       "      <td>0.979444</td>\n",
       "      <td>0.980278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, Vicuna-7B)</th>\n",
       "      <td>0.939167</td>\n",
       "      <td>0.940556</td>\n",
       "      <td>0.942222</td>\n",
       "      <td>0.946111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.998889</td>\n",
       "      <td>1.013333</td>\n",
       "      <td>1.012500</td>\n",
       "      <td>1.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>2.812500</td>\n",
       "      <td>2.818056</td>\n",
       "      <td>2.806111</td>\n",
       "      <td>2.812778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, Vicuna-7B)</th>\n",
       "      <td>2.525833</td>\n",
       "      <td>2.542222</td>\n",
       "      <td>2.530556</td>\n",
       "      <td>2.535278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, Vicuna-7B)</th>\n",
       "      <td>2.905278</td>\n",
       "      <td>2.891389</td>\n",
       "      <td>2.887778</td>\n",
       "      <td>2.897500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, Vicuna-7B)</th>\n",
       "      <td>1.950000</td>\n",
       "      <td>1.893333</td>\n",
       "      <td>1.904722</td>\n",
       "      <td>1.901389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>1.098333</td>\n",
       "      <td>1.123889</td>\n",
       "      <td>1.117500</td>\n",
       "      <td>1.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (self-attn QVO, FlanT5-XL)</th>\n",
       "      <td>3.306944</td>\n",
       "      <td>3.290833</td>\n",
       "      <td>3.093056</td>\n",
       "      <td>3.375278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QVO, FlanT5-XL)</th>\n",
       "      <td>3.411389</td>\n",
       "      <td>3.403889</td>\n",
       "      <td>3.509444</td>\n",
       "      <td>3.444722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QV, FlanT5-XL)</th>\n",
       "      <td>3.359444</td>\n",
       "      <td>3.347222</td>\n",
       "      <td>3.448889</td>\n",
       "      <td>3.338056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           r = 1     r = 2     r = 4     r = 8\n",
       "LLM LoRA (ffn, FlanT5-XL)               3.658333  2.751944  3.770833  3.788611\n",
       "LLM LoRA (attn, FlanT5-XL)              2.425278  2.679722  2.674722  2.923889\n",
       "LLM LoRA (all, FlanT5-XL)               3.126389  3.912778  2.881111  3.175556\n",
       "Q-Former LoRA (ffn, FlanT5-XL)          3.606667  3.638889  3.643056  3.634167\n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    2.804167  2.840833  2.611944  2.830278\n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   3.443611  3.438889  3.451389  3.432500\n",
       "Q-Former LoRA (all, FlanT5-XL)          1.813889  1.849444  1.848889  1.846667\n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  3.884722  3.280278  4.500278  3.882778\n",
       "LLM LoRA (ffn, Vicuna-7B)               0.968333  0.981667  0.979444  0.980278\n",
       "LLM LoRA (attn, Vicuna-7B)              0.939167  0.940556  0.942222  0.946111\n",
       "LLM LoRA (all, Vicuna-7B)               0.998889  1.013333  1.012500  1.013611\n",
       "Q-Former LoRA (ffn, Vicuna-7B)          2.812500  2.818056  2.806111  2.812778\n",
       "Q-Former LoRA (self-attn, Vicuna-7B)    2.525833  2.542222  2.530556  2.535278\n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)   2.905278  2.891389  2.887778  2.897500\n",
       "Q-Former LoRA (all, Vicuna-7B)          1.950000  1.893333  1.904722  1.901389\n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)  1.098333  1.123889  1.117500  1.105000\n",
       "Q-Former (self-attn QVO, FlanT5-XL)     3.306944  3.290833  3.093056  3.375278\n",
       "Q-Former (cross-attn QVO, FlanT5-XL)    3.411389  3.403889  3.509444  3.444722\n",
       "Q-Former (cross-attn QV, FlanT5-XL)     3.359444  3.347222  3.448889  3.338056"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_time_df.to_csv(f'{dataset}/training_time.csv', index=True)\n",
    "training_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r = 1</th>\n",
       "      <th>r = 2</th>\n",
       "      <th>r = 4</th>\n",
       "      <th>r = 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>0.565313</td>\n",
       "      <td>0.579909</td>\n",
       "      <td>0.582727</td>\n",
       "      <td>0.585480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, FlanT5-XL)</th>\n",
       "      <td>0.562190</td>\n",
       "      <td>0.564655</td>\n",
       "      <td>0.563664</td>\n",
       "      <td>0.564733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>0.603858</td>\n",
       "      <td>0.604720</td>\n",
       "      <td>0.607045</td>\n",
       "      <td>0.613300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>0.557320</td>\n",
       "      <td>0.562367</td>\n",
       "      <td>0.562940</td>\n",
       "      <td>0.561567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, FlanT5-XL)</th>\n",
       "      <td>0.541683</td>\n",
       "      <td>0.548708</td>\n",
       "      <td>0.550345</td>\n",
       "      <td>0.546692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, FlanT5-XL)</th>\n",
       "      <td>0.532133</td>\n",
       "      <td>0.531407</td>\n",
       "      <td>0.533367</td>\n",
       "      <td>0.530473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, FlanT5-XL)</th>\n",
       "      <td>0.600543</td>\n",
       "      <td>0.612486</td>\n",
       "      <td>0.612314</td>\n",
       "      <td>0.611457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>0.692585</td>\n",
       "      <td>0.691209</td>\n",
       "      <td>0.695280</td>\n",
       "      <td>0.692246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.561125</td>\n",
       "      <td>0.568975</td>\n",
       "      <td>0.567650</td>\n",
       "      <td>0.568075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, Vicuna-7B)</th>\n",
       "      <td>0.544150</td>\n",
       "      <td>0.544925</td>\n",
       "      <td>0.545925</td>\n",
       "      <td>0.548175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.578925</td>\n",
       "      <td>0.587125</td>\n",
       "      <td>0.586725</td>\n",
       "      <td>0.587450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.931500</td>\n",
       "      <td>0.933257</td>\n",
       "      <td>0.929450</td>\n",
       "      <td>0.931336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, Vicuna-7B)</th>\n",
       "      <td>0.900777</td>\n",
       "      <td>0.906531</td>\n",
       "      <td>0.902508</td>\n",
       "      <td>0.904023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, Vicuna-7B)</th>\n",
       "      <td>0.898160</td>\n",
       "      <td>0.893767</td>\n",
       "      <td>0.892620</td>\n",
       "      <td>0.895593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, Vicuna-7B)</th>\n",
       "      <td>1.004467</td>\n",
       "      <td>0.975422</td>\n",
       "      <td>0.981333</td>\n",
       "      <td>0.979389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.636425</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>0.647575</td>\n",
       "      <td>0.640200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (self-attn QVO, FlanT5-XL)</th>\n",
       "      <td>0.547529</td>\n",
       "      <td>0.544821</td>\n",
       "      <td>0.551477</td>\n",
       "      <td>0.558750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QVO, FlanT5-XL)</th>\n",
       "      <td>0.527187</td>\n",
       "      <td>0.526020</td>\n",
       "      <td>0.542307</td>\n",
       "      <td>0.532253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QV, FlanT5-XL)</th>\n",
       "      <td>0.519147</td>\n",
       "      <td>0.517247</td>\n",
       "      <td>0.532940</td>\n",
       "      <td>0.515873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           r = 1     r = 2     r = 4     r = 8\n",
       "LLM LoRA (ffn, FlanT5-XL)               0.565313  0.579909  0.582727  0.585480\n",
       "LLM LoRA (attn, FlanT5-XL)              0.562190  0.564655  0.563664  0.564733\n",
       "LLM LoRA (all, FlanT5-XL)               0.603858  0.604720  0.607045  0.613300\n",
       "Q-Former LoRA (ffn, FlanT5-XL)          0.557320  0.562367  0.562940  0.561567\n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    0.541683  0.548708  0.550345  0.546692\n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   0.532133  0.531407  0.533367  0.530473\n",
       "Q-Former LoRA (all, FlanT5-XL)          0.600543  0.612486  0.612314  0.611457\n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  0.692585  0.691209  0.695280  0.692246\n",
       "LLM LoRA (ffn, Vicuna-7B)               0.561125  0.568975  0.567650  0.568075\n",
       "LLM LoRA (attn, Vicuna-7B)              0.544150  0.544925  0.545925  0.548175\n",
       "LLM LoRA (all, Vicuna-7B)               0.578925  0.587125  0.586725  0.587450\n",
       "Q-Former LoRA (ffn, Vicuna-7B)          0.931500  0.933257  0.929450  0.931336\n",
       "Q-Former LoRA (self-attn, Vicuna-7B)    0.900777  0.906531  0.902508  0.904023\n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)   0.898160  0.893767  0.892620  0.895593\n",
       "Q-Former LoRA (all, Vicuna-7B)          1.004467  0.975422  0.981333  0.979389\n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)  0.636425  0.651200  0.647575  0.640200\n",
       "Q-Former (self-attn QVO, FlanT5-XL)     0.547529  0.544821  0.551477  0.558750\n",
       "Q-Former (cross-attn QVO, FlanT5-XL)    0.527187  0.526020  0.542307  0.532253\n",
       "Q-Former (cross-attn QV, FlanT5-XL)     0.519147  0.517247  0.532940  0.515873"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_per_it_df.to_csv(f'{dataset}/time_per_it.csv', index=True)\n",
    "time_per_it_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r = 1</th>\n",
       "      <th>r = 2</th>\n",
       "      <th>r = 4</th>\n",
       "      <th>r = 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>1032192</td>\n",
       "      <td>2064384</td>\n",
       "      <td>4128768</td>\n",
       "      <td>8257536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, FlanT5-XL)</th>\n",
       "      <td>589824</td>\n",
       "      <td>1179648</td>\n",
       "      <td>2359296</td>\n",
       "      <td>4718592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>1622016</td>\n",
       "      <td>3244032</td>\n",
       "      <td>6488064</td>\n",
       "      <td>12976128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>184320</td>\n",
       "      <td>368640</td>\n",
       "      <td>737280</td>\n",
       "      <td>1474560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, FlanT5-XL)</th>\n",
       "      <td>36864</td>\n",
       "      <td>73728</td>\n",
       "      <td>147456</td>\n",
       "      <td>294912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, FlanT5-XL)</th>\n",
       "      <td>44544</td>\n",
       "      <td>89088</td>\n",
       "      <td>178176</td>\n",
       "      <td>356352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, FlanT5-XL)</th>\n",
       "      <td>265728</td>\n",
       "      <td>531456</td>\n",
       "      <td>1062912</td>\n",
       "      <td>2125824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>1887744</td>\n",
       "      <td>3775488</td>\n",
       "      <td>7550976</td>\n",
       "      <td>15101952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>1449984</td>\n",
       "      <td>2899968</td>\n",
       "      <td>5799936</td>\n",
       "      <td>11599872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, Vicuna-7B)</th>\n",
       "      <td>524288</td>\n",
       "      <td>1048576</td>\n",
       "      <td>2097152</td>\n",
       "      <td>4194304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>1974272</td>\n",
       "      <td>3948544</td>\n",
       "      <td>7897088</td>\n",
       "      <td>15794176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>184320</td>\n",
       "      <td>368640</td>\n",
       "      <td>737280</td>\n",
       "      <td>1474560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, Vicuna-7B)</th>\n",
       "      <td>36864</td>\n",
       "      <td>73728</td>\n",
       "      <td>147456</td>\n",
       "      <td>294912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, Vicuna-7B)</th>\n",
       "      <td>44544</td>\n",
       "      <td>89088</td>\n",
       "      <td>178176</td>\n",
       "      <td>356352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, Vicuna-7B)</th>\n",
       "      <td>265728</td>\n",
       "      <td>531456</td>\n",
       "      <td>1062912</td>\n",
       "      <td>2125824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>2240000</td>\n",
       "      <td>4480000</td>\n",
       "      <td>8960000</td>\n",
       "      <td>17920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (self-attn QVO, FlanT5-XL)</th>\n",
       "      <td>55296</td>\n",
       "      <td>110592</td>\n",
       "      <td>221184</td>\n",
       "      <td>442368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QVO, FlanT5-XL)</th>\n",
       "      <td>31488</td>\n",
       "      <td>62976</td>\n",
       "      <td>125952</td>\n",
       "      <td>251904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QV, FlanT5-XL)</th>\n",
       "      <td>22272</td>\n",
       "      <td>44544</td>\n",
       "      <td>89088</td>\n",
       "      <td>178176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          r = 1    r = 2    r = 4     r = 8\n",
       "LLM LoRA (ffn, FlanT5-XL)               1032192  2064384  4128768   8257536\n",
       "LLM LoRA (attn, FlanT5-XL)               589824  1179648  2359296   4718592\n",
       "LLM LoRA (all, FlanT5-XL)               1622016  3244032  6488064  12976128\n",
       "Q-Former LoRA (ffn, FlanT5-XL)           184320   368640   737280   1474560\n",
       "Q-Former LoRA (self-attn, FlanT5-XL)      36864    73728   147456    294912\n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)     44544    89088   178176    356352\n",
       "Q-Former LoRA (all, FlanT5-XL)           265728   531456  1062912   2125824\n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  1887744  3775488  7550976  15101952\n",
       "LLM LoRA (ffn, Vicuna-7B)               1449984  2899968  5799936  11599872\n",
       "LLM LoRA (attn, Vicuna-7B)               524288  1048576  2097152   4194304\n",
       "LLM LoRA (all, Vicuna-7B)               1974272  3948544  7897088  15794176\n",
       "Q-Former LoRA (ffn, Vicuna-7B)           184320   368640   737280   1474560\n",
       "Q-Former LoRA (self-attn, Vicuna-7B)      36864    73728   147456    294912\n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)     44544    89088   178176    356352\n",
       "Q-Former LoRA (all, Vicuna-7B)           265728   531456  1062912   2125824\n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)  2240000  4480000  8960000  17920000\n",
       "Q-Former (self-attn QVO, FlanT5-XL)       55296   110592   221184    442368\n",
       "Q-Former (cross-attn QVO, FlanT5-XL)      31488    62976   125952    251904\n",
       "Q-Former (cross-attn QV, FlanT5-XL)       22272    44544    89088    178176"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_params_df.astype(int).to_csv(f'{dataset}/trainable_params.csv', index=True)\n",
    "trainable_params_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r = 1</th>\n",
       "      <th>r = 2</th>\n",
       "      <th>r = 4</th>\n",
       "      <th>r = 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>[0.2437, 0.1342, 0.1701, 0.1755, 0.1314, 0.052...</td>\n",
       "      <td>[0.2472, 0.1628, 0.1447, 0.1174, 0.1256, 0.041...</td>\n",
       "      <td>[0.2428, 0.1389, 0.1275, 0.1507, 0.1255, 0.047...</td>\n",
       "      <td>[0.2165, 0.1376, 0.1535, 0.0659, 0.1308, 0.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, FlanT5-XL)</th>\n",
       "      <td>[0.2394, 0.1274, 0.1313, 0.1086, 0.1423, 0.057...</td>\n",
       "      <td>[0.221, 0.149, 0.1634, 0.0836, 0.1367, 0.051, ...</td>\n",
       "      <td>[0.2467, 0.15, 0.1737, 0.078, 0.1414, 0.0486, ...</td>\n",
       "      <td>[0.2451, 0.1482, 0.1613, 0.136, 0.1171, 0.0473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>[0.2185, 0.1598, 0.1388, 0.069, 0.1234, 0.0407...</td>\n",
       "      <td>[0.2421, 0.1484, 0.1616, 0.0975, 0.1291, 0.049...</td>\n",
       "      <td>[0.2334, 0.1187, 0.1774, 0.1301, 0.1221, 0.042...</td>\n",
       "      <td>[0.2047, 0.1421, 0.1705, 0.0805, 0.1267, 0.039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>[0.2295, 0.1339, 0.1704, 0.0791, 0.1271, 0.043...</td>\n",
       "      <td>[0.2295, 0.1339, 0.1704, 0.0791, 0.1271, 0.043...</td>\n",
       "      <td>[0.2295, 0.1339, 0.1704, 0.0791, 0.1271, 0.043...</td>\n",
       "      <td>[0.2295, 0.1339, 0.1704, 0.0791, 0.1271, 0.043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, FlanT5-XL)</th>\n",
       "      <td>[0.2207, 0.1499, 0.1777, 0.1229, 0.12, 0.0452,...</td>\n",
       "      <td>[0.2207, 0.1499, 0.1777, 0.1229, 0.12, 0.0452,...</td>\n",
       "      <td>[0.2207, 0.1855, 0.1669, 0.0937, 0.1488, 0.049...</td>\n",
       "      <td>[0.2207, 0.1499, 0.1777, 0.1229, 0.12, 0.0452,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, FlanT5-XL)</th>\n",
       "      <td>[0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...</td>\n",
       "      <td>[0.2083, 0.1616, 0.2149, 0.0923, 0.1291, 0.049...</td>\n",
       "      <td>[0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...</td>\n",
       "      <td>[0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, FlanT5-XL)</th>\n",
       "      <td>[0.2433, 0.1378, 0.1676, 0.125, 0.1292, 0.0475...</td>\n",
       "      <td>[0.2433, 0.1378, 0.1676, 0.125, 0.1292, 0.0475...</td>\n",
       "      <td>[0.2433, 0.1378, 0.1676, 0.125, 0.1292, 0.0475...</td>\n",
       "      <td>[0.2433, 0.1378, 0.1676, 0.125, 0.1292, 0.0475...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>[0.2424, 0.1573, 0.1535, 0.1062, 0.1225, 0.043...</td>\n",
       "      <td>[0.2302, 0.1708, 0.2173, 0.0773, 0.1288, 0.043...</td>\n",
       "      <td>[0.242, 0.1736, 0.1802, 0.1149, 0.1158, 0.0471...</td>\n",
       "      <td>[0.2171, 0.1627, 0.1578, 0.1091, 0.1205, 0.044...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>[0.1046, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.0393, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.0411, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.0435, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, Vicuna-7B)</th>\n",
       "      <td>[0.0822, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.0926, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.0406, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.0945, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>[0.1128, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.0488, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.0896, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.093, nan, nan, nan, nan, nan, nan, nan, nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>[0.0992, 0.0549, 0.0325, 0.0405, 0.0525, 0.031...</td>\n",
       "      <td>[0.0992, 0.0549, 0.0325, 0.0405, 0.0525, 0.031...</td>\n",
       "      <td>[0.0992, 0.0549, 0.0325, 0.0405, 0.0525, 0.031...</td>\n",
       "      <td>[0.0992, 0.0549, 0.0325, 0.0405, 0.0525, 0.031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, Vicuna-7B)</th>\n",
       "      <td>[0.0796, 0.0461, 0.0352, 0.0386, 0.0446, 0.034...</td>\n",
       "      <td>[0.0796, 0.0461, 0.0352, 0.0386, 0.0446, 0.034...</td>\n",
       "      <td>[0.0796, 0.0461, 0.0352, 0.0386, 0.0446, 0.034...</td>\n",
       "      <td>[0.0796, 0.0461, 0.0352, 0.0386, 0.0446, 0.034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, Vicuna-7B)</th>\n",
       "      <td>[0.0969, 0.0571, 0.0322, 0.0393, 0.044, 0.0293...</td>\n",
       "      <td>[0.0969, 0.0571, 0.0322, 0.0393, 0.044, 0.0293...</td>\n",
       "      <td>[0.0969, 0.0571, 0.0322, 0.0393, 0.044, 0.0293...</td>\n",
       "      <td>[0.0969, 0.0571, 0.0322, 0.0393, 0.044, 0.0293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, Vicuna-7B)</th>\n",
       "      <td>[0.0826, 0.0508, 0.0319, 0.0393, 0.0522, 0.034...</td>\n",
       "      <td>[0.0826, 0.0508, 0.0319, 0.0393, 0.0522, 0.034...</td>\n",
       "      <td>[0.0826, 0.0508, 0.0319, 0.0393, 0.0522, 0.034...</td>\n",
       "      <td>[0.0826, 0.0508, 0.0319, 0.0393, 0.0522, 0.034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>[0.0548, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.0936, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.0848, nan, nan, nan, nan, nan, nan, nan, na...</td>\n",
       "      <td>[0.096, nan, nan, nan, nan, nan, nan, nan, nan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (self-attn QVO, FlanT5-XL)</th>\n",
       "      <td>[0.2519, 0.1178, 0.1946, 0.1308, 0.1204, 0.049...</td>\n",
       "      <td>[0.2519, 0.1178, 0.1946, 0.1308, 0.1204, 0.049...</td>\n",
       "      <td>[0.2519, 0.1565, 0.2235, 0.1095, 0.1266, 0.049...</td>\n",
       "      <td>[0.2519, 0.1178, 0.1946, 0.1308, 0.1204, 0.049...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QVO, FlanT5-XL)</th>\n",
       "      <td>[0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...</td>\n",
       "      <td>[0.2083, 0.1616, 0.2149, 0.0923, 0.1291, 0.049...</td>\n",
       "      <td>[0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...</td>\n",
       "      <td>[0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QV, FlanT5-XL)</th>\n",
       "      <td>[0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...</td>\n",
       "      <td>[0.2083, 0.1616, 0.2149, 0.0923, 0.1291, 0.049...</td>\n",
       "      <td>[0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...</td>\n",
       "      <td>[0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    r = 1  \\\n",
       "LLM LoRA (ffn, FlanT5-XL)               [0.2437, 0.1342, 0.1701, 0.1755, 0.1314, 0.052...   \n",
       "LLM LoRA (attn, FlanT5-XL)              [0.2394, 0.1274, 0.1313, 0.1086, 0.1423, 0.057...   \n",
       "LLM LoRA (all, FlanT5-XL)               [0.2185, 0.1598, 0.1388, 0.069, 0.1234, 0.0407...   \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [0.2295, 0.1339, 0.1704, 0.0791, 0.1271, 0.043...   \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [0.2207, 0.1499, 0.1777, 0.1229, 0.12, 0.0452,...   \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...   \n",
       "Q-Former LoRA (all, FlanT5-XL)          [0.2433, 0.1378, 0.1676, 0.125, 0.1292, 0.0475...   \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [0.2424, 0.1573, 0.1535, 0.1062, 0.1225, 0.043...   \n",
       "LLM LoRA (ffn, Vicuna-7B)               [0.1046, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "LLM LoRA (attn, Vicuna-7B)              [0.0822, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "LLM LoRA (all, Vicuna-7B)               [0.1128, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "Q-Former LoRA (ffn, Vicuna-7B)          [0.0992, 0.0549, 0.0325, 0.0405, 0.0525, 0.031...   \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)    [0.0796, 0.0461, 0.0352, 0.0386, 0.0446, 0.034...   \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)   [0.0969, 0.0571, 0.0322, 0.0393, 0.044, 0.0293...   \n",
       "Q-Former LoRA (all, Vicuna-7B)          [0.0826, 0.0508, 0.0319, 0.0393, 0.0522, 0.034...   \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)  [0.0548, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "Q-Former (self-attn QVO, FlanT5-XL)     [0.2519, 0.1178, 0.1946, 0.1308, 0.1204, 0.049...   \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)    [0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...   \n",
       "Q-Former (cross-attn QV, FlanT5-XL)     [0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...   \n",
       "\n",
       "                                                                                    r = 2  \\\n",
       "LLM LoRA (ffn, FlanT5-XL)               [0.2472, 0.1628, 0.1447, 0.1174, 0.1256, 0.041...   \n",
       "LLM LoRA (attn, FlanT5-XL)              [0.221, 0.149, 0.1634, 0.0836, 0.1367, 0.051, ...   \n",
       "LLM LoRA (all, FlanT5-XL)               [0.2421, 0.1484, 0.1616, 0.0975, 0.1291, 0.049...   \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [0.2295, 0.1339, 0.1704, 0.0791, 0.1271, 0.043...   \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [0.2207, 0.1499, 0.1777, 0.1229, 0.12, 0.0452,...   \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [0.2083, 0.1616, 0.2149, 0.0923, 0.1291, 0.049...   \n",
       "Q-Former LoRA (all, FlanT5-XL)          [0.2433, 0.1378, 0.1676, 0.125, 0.1292, 0.0475...   \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [0.2302, 0.1708, 0.2173, 0.0773, 0.1288, 0.043...   \n",
       "LLM LoRA (ffn, Vicuna-7B)               [0.0393, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "LLM LoRA (attn, Vicuna-7B)              [0.0926, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "LLM LoRA (all, Vicuna-7B)               [0.0488, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "Q-Former LoRA (ffn, Vicuna-7B)          [0.0992, 0.0549, 0.0325, 0.0405, 0.0525, 0.031...   \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)    [0.0796, 0.0461, 0.0352, 0.0386, 0.0446, 0.034...   \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)   [0.0969, 0.0571, 0.0322, 0.0393, 0.044, 0.0293...   \n",
       "Q-Former LoRA (all, Vicuna-7B)          [0.0826, 0.0508, 0.0319, 0.0393, 0.0522, 0.034...   \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)  [0.0936, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "Q-Former (self-attn QVO, FlanT5-XL)     [0.2519, 0.1178, 0.1946, 0.1308, 0.1204, 0.049...   \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)    [0.2083, 0.1616, 0.2149, 0.0923, 0.1291, 0.049...   \n",
       "Q-Former (cross-attn QV, FlanT5-XL)     [0.2083, 0.1616, 0.2149, 0.0923, 0.1291, 0.049...   \n",
       "\n",
       "                                                                                    r = 4  \\\n",
       "LLM LoRA (ffn, FlanT5-XL)               [0.2428, 0.1389, 0.1275, 0.1507, 0.1255, 0.047...   \n",
       "LLM LoRA (attn, FlanT5-XL)              [0.2467, 0.15, 0.1737, 0.078, 0.1414, 0.0486, ...   \n",
       "LLM LoRA (all, FlanT5-XL)               [0.2334, 0.1187, 0.1774, 0.1301, 0.1221, 0.042...   \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [0.2295, 0.1339, 0.1704, 0.0791, 0.1271, 0.043...   \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [0.2207, 0.1855, 0.1669, 0.0937, 0.1488, 0.049...   \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...   \n",
       "Q-Former LoRA (all, FlanT5-XL)          [0.2433, 0.1378, 0.1676, 0.125, 0.1292, 0.0475...   \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [0.242, 0.1736, 0.1802, 0.1149, 0.1158, 0.0471...   \n",
       "LLM LoRA (ffn, Vicuna-7B)               [0.0411, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "LLM LoRA (attn, Vicuna-7B)              [0.0406, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "LLM LoRA (all, Vicuna-7B)               [0.0896, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "Q-Former LoRA (ffn, Vicuna-7B)          [0.0992, 0.0549, 0.0325, 0.0405, 0.0525, 0.031...   \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)    [0.0796, 0.0461, 0.0352, 0.0386, 0.0446, 0.034...   \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)   [0.0969, 0.0571, 0.0322, 0.0393, 0.044, 0.0293...   \n",
       "Q-Former LoRA (all, Vicuna-7B)          [0.0826, 0.0508, 0.0319, 0.0393, 0.0522, 0.034...   \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)  [0.0848, nan, nan, nan, nan, nan, nan, nan, na...   \n",
       "Q-Former (self-attn QVO, FlanT5-XL)     [0.2519, 0.1565, 0.2235, 0.1095, 0.1266, 0.049...   \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)    [0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...   \n",
       "Q-Former (cross-attn QV, FlanT5-XL)     [0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...   \n",
       "\n",
       "                                                                                    r = 8  \n",
       "LLM LoRA (ffn, FlanT5-XL)               [0.2165, 0.1376, 0.1535, 0.0659, 0.1308, 0.046...  \n",
       "LLM LoRA (attn, FlanT5-XL)              [0.2451, 0.1482, 0.1613, 0.136, 0.1171, 0.0473...  \n",
       "LLM LoRA (all, FlanT5-XL)               [0.2047, 0.1421, 0.1705, 0.0805, 0.1267, 0.039...  \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [0.2295, 0.1339, 0.1704, 0.0791, 0.1271, 0.043...  \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [0.2207, 0.1499, 0.1777, 0.1229, 0.12, 0.0452,...  \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...  \n",
       "Q-Former LoRA (all, FlanT5-XL)          [0.2433, 0.1378, 0.1676, 0.125, 0.1292, 0.0475...  \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [0.2171, 0.1627, 0.1578, 0.1091, 0.1205, 0.044...  \n",
       "LLM LoRA (ffn, Vicuna-7B)               [0.0435, nan, nan, nan, nan, nan, nan, nan, na...  \n",
       "LLM LoRA (attn, Vicuna-7B)              [0.0945, nan, nan, nan, nan, nan, nan, nan, na...  \n",
       "LLM LoRA (all, Vicuna-7B)               [0.093, nan, nan, nan, nan, nan, nan, nan, nan...  \n",
       "Q-Former LoRA (ffn, Vicuna-7B)          [0.0992, 0.0549, 0.0325, 0.0405, 0.0525, 0.031...  \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)    [0.0796, 0.0461, 0.0352, 0.0386, 0.0446, 0.034...  \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)   [0.0969, 0.0571, 0.0322, 0.0393, 0.044, 0.0293...  \n",
       "Q-Former LoRA (all, Vicuna-7B)          [0.0826, 0.0508, 0.0319, 0.0393, 0.0522, 0.034...  \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)  [0.096, nan, nan, nan, nan, nan, nan, nan, nan...  \n",
       "Q-Former (self-attn QVO, FlanT5-XL)     [0.2519, 0.1178, 0.1946, 0.1308, 0.1204, 0.049...  \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)    [0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...  \n",
       "Q-Former (cross-attn QV, FlanT5-XL)     [0.2083, 0.1379, 0.1765, 0.0972, 0.1229, 0.046...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_hist_df.to_csv(f'{dataset}/loss_hist.csv', index=True)\n",
    "loss_hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r = 1</th>\n",
       "      <th>r = 2</th>\n",
       "      <th>r = 4</th>\n",
       "      <th>r = 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>[0.154, 0.116, 0.098, 0.089, 0.082, 0.079, 0.0...</td>\n",
       "      <td>[0.156, 0.113, 0.096, 0.087, 0.081, 0.078, 0.0...</td>\n",
       "      <td>[0.153, 0.115, 0.1, 0.09, 0.082, 0.079, 0.075,...</td>\n",
       "      <td>[0.156, 0.117, 0.099, 0.088, 0.082, 0.077, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, FlanT5-XL)</th>\n",
       "      <td>[0.159, 0.117, 0.1, 0.09, 0.084, 0.079, 0.076,...</td>\n",
       "      <td>[0.155, 0.114, 0.097, 0.088, 0.081, 0.076, 0.0...</td>\n",
       "      <td>[0.154, 0.113, 0.094, 0.087, 0.082, 0.077, 0.0...</td>\n",
       "      <td>[0.159, 0.117, 0.1, 0.091, 0.084, 0.079, 0.076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>[0.158, 0.115, 0.099, 0.088, 0.082, 0.079, 0.0...</td>\n",
       "      <td>[0.151, 0.112, 0.095, 0.087, 0.08, 0.077, 0.07...</td>\n",
       "      <td>[0.151, 0.114, 0.097, 0.089, 0.082, 0.077, 0.0...</td>\n",
       "      <td>[0.158, 0.115, 0.099, 0.089, 0.083, 0.077, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>[0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[0.187, 0.154, 0.141, 0.136, 0.131, 0.125, 0.1...</td>\n",
       "      <td>[0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, FlanT5-XL)</th>\n",
       "      <td>[0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "      <td>[0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, FlanT5-XL)</th>\n",
       "      <td>[0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...</td>\n",
       "      <td>[0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...</td>\n",
       "      <td>[0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...</td>\n",
       "      <td>[0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, FlanT5-XL)</th>\n",
       "      <td>[0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...</td>\n",
       "      <td>[0.186, 0.155, 0.142, 0.135, 0.13, 0.125, 0.12...</td>\n",
       "      <td>[0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...</td>\n",
       "      <td>[0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>[0.155, 0.129, 0.12, 0.113, 0.11, 0.103, 0.1, ...</td>\n",
       "      <td>[0.154, 0.131, 0.118, 0.237, 0.105, 0.101, 0.0...</td>\n",
       "      <td>[0.155, 0.126, 0.117, 0.111, 0.104, 0.099, 0.0...</td>\n",
       "      <td>[0.154, 0.13, 0.123, 0.112, 0.107, 0.101, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (self-attn QVO, FlanT5-XL)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QVO, FlanT5-XL)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QV, FlanT5-XL)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    r = 1  \\\n",
       "LLM LoRA (ffn, FlanT5-XL)               [0.154, 0.116, 0.098, 0.089, 0.082, 0.079, 0.0...   \n",
       "LLM LoRA (attn, FlanT5-XL)              [0.159, 0.117, 0.1, 0.09, 0.084, 0.079, 0.076,...   \n",
       "LLM LoRA (all, FlanT5-XL)               [0.158, 0.115, 0.099, 0.088, 0.082, 0.079, 0.0...   \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...   \n",
       "Q-Former LoRA (all, FlanT5-XL)          [0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...   \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [0.155, 0.129, 0.12, 0.113, 0.11, 0.103, 0.1, ...   \n",
       "LLM LoRA (ffn, Vicuna-7B)                                                             0.0   \n",
       "LLM LoRA (attn, Vicuna-7B)                                                            0.0   \n",
       "LLM LoRA (all, Vicuna-7B)                                                             0.0   \n",
       "Q-Former LoRA (ffn, Vicuna-7B)                                                        0.0   \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)                                                  0.0   \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)                                                 0.0   \n",
       "Q-Former LoRA (all, Vicuna-7B)                                                        0.0   \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)                                                0.0   \n",
       "Q-Former (self-attn QVO, FlanT5-XL)                                                   0.0   \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)                                                  0.0   \n",
       "Q-Former (cross-attn QV, FlanT5-XL)                                                   0.0   \n",
       "\n",
       "                                                                                    r = 2  \\\n",
       "LLM LoRA (ffn, FlanT5-XL)               [0.156, 0.113, 0.096, 0.087, 0.081, 0.078, 0.0...   \n",
       "LLM LoRA (attn, FlanT5-XL)              [0.155, 0.114, 0.097, 0.088, 0.081, 0.076, 0.0...   \n",
       "LLM LoRA (all, FlanT5-XL)               [0.151, 0.112, 0.095, 0.087, 0.08, 0.077, 0.07...   \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...   \n",
       "Q-Former LoRA (all, FlanT5-XL)          [0.186, 0.155, 0.142, 0.135, 0.13, 0.125, 0.12...   \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [0.154, 0.131, 0.118, 0.237, 0.105, 0.101, 0.0...   \n",
       "LLM LoRA (ffn, Vicuna-7B)                                                             0.0   \n",
       "LLM LoRA (attn, Vicuna-7B)                                                            0.0   \n",
       "LLM LoRA (all, Vicuna-7B)                                                             0.0   \n",
       "Q-Former LoRA (ffn, Vicuna-7B)                                                        0.0   \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)                                                  0.0   \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)                                                 0.0   \n",
       "Q-Former LoRA (all, Vicuna-7B)                                                        0.0   \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)                                                0.0   \n",
       "Q-Former (self-attn QVO, FlanT5-XL)                                                   0.0   \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)                                                  0.0   \n",
       "Q-Former (cross-attn QV, FlanT5-XL)                                                   0.0   \n",
       "\n",
       "                                                                                    r = 4  \\\n",
       "LLM LoRA (ffn, FlanT5-XL)               [0.153, 0.115, 0.1, 0.09, 0.082, 0.079, 0.075,...   \n",
       "LLM LoRA (attn, FlanT5-XL)              [0.154, 0.113, 0.094, 0.087, 0.082, 0.077, 0.0...   \n",
       "LLM LoRA (all, FlanT5-XL)               [0.151, 0.114, 0.097, 0.089, 0.082, 0.077, 0.0...   \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [0.187, 0.154, 0.141, 0.136, 0.131, 0.125, 0.1...   \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...   \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...   \n",
       "Q-Former LoRA (all, FlanT5-XL)          [0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...   \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [0.155, 0.126, 0.117, 0.111, 0.104, 0.099, 0.0...   \n",
       "LLM LoRA (ffn, Vicuna-7B)                                                             0.0   \n",
       "LLM LoRA (attn, Vicuna-7B)                                                            0.0   \n",
       "LLM LoRA (all, Vicuna-7B)                                                             0.0   \n",
       "Q-Former LoRA (ffn, Vicuna-7B)                                                        0.0   \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)                                                  0.0   \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)                                                 0.0   \n",
       "Q-Former LoRA (all, Vicuna-7B)                                                        0.0   \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)                                                0.0   \n",
       "Q-Former (self-attn QVO, FlanT5-XL)                                                   0.0   \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)                                                  0.0   \n",
       "Q-Former (cross-attn QV, FlanT5-XL)                                                   0.0   \n",
       "\n",
       "                                                                                    r = 8  \n",
       "LLM LoRA (ffn, FlanT5-XL)               [0.156, 0.117, 0.099, 0.088, 0.082, 0.077, 0.0...  \n",
       "LLM LoRA (attn, FlanT5-XL)              [0.159, 0.117, 0.1, 0.091, 0.084, 0.079, 0.076...  \n",
       "LLM LoRA (all, FlanT5-XL)               [0.158, 0.115, 0.099, 0.089, 0.083, 0.077, 0.0...  \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [0.163, 0.133, 0.12, 0.112, 0.105, 0.101, 0.09...  \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [0.164, 0.131, 0.12, 0.112, 0.105, 0.101, 0.09...  \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [0.168, 0.156, 0.14, 0.132, 0.139, 0.123, 0.11...  \n",
       "Q-Former LoRA (all, FlanT5-XL)          [0.166, 0.133, 0.121, 0.112, 0.106, 0.101, 0.0...  \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [0.154, 0.13, 0.123, 0.112, 0.107, 0.101, 0.09...  \n",
       "LLM LoRA (ffn, Vicuna-7B)                                                             0.0  \n",
       "LLM LoRA (attn, Vicuna-7B)                                                            0.0  \n",
       "LLM LoRA (all, Vicuna-7B)                                                             0.0  \n",
       "Q-Former LoRA (ffn, Vicuna-7B)                                                        0.0  \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)                                                  0.0  \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)                                                 0.0  \n",
       "Q-Former LoRA (all, Vicuna-7B)                                                        0.0  \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)                                                0.0  \n",
       "Q-Former (self-attn QVO, FlanT5-XL)                                                   0.0  \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)                                                  0.0  \n",
       "Q-Former (cross-attn QV, FlanT5-XL)                                                   0.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_epoch_df.to_csv(f'{dataset}/loss_per_epoch.csv', index=True)\n",
    "loss_per_epoch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r = 1</th>\n",
       "      <th>r = 2</th>\n",
       "      <th>r = 4</th>\n",
       "      <th>r = 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>[66.03863204559848, 68.84103863204561, 68.5877...</td>\n",
       "      <td>[67.19442685243826, 68.1760607979734, 69.49018...</td>\n",
       "      <td>[66.07029765674477, 66.2444585180494, 67.47941...</td>\n",
       "      <td>[66.2444585180494, 67.30525649145028, 70.55098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, FlanT5-XL)</th>\n",
       "      <td>[63.01456618112729, 67.32108929702343, 68.0018...</td>\n",
       "      <td>[65.83280557314755, 67.59024699176695, 69.5851...</td>\n",
       "      <td>[64.70867637745408, 67.25775807473084, 68.2552...</td>\n",
       "      <td>[65.84863837872071, 67.2102596580114, 68.90436...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>[66.95693476884104, 66.35528815706144, 70.4243...</td>\n",
       "      <td>[67.79607346421786, 68.99936668777707, 68.8568...</td>\n",
       "      <td>[66.56111462951235, 68.09689677010766, 71.1526...</td>\n",
       "      <td>[65.880303989867, 67.97023432552248, 68.936035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, FlanT5-XL)</th>\n",
       "      <td>[65.95946801773275, 65.94363521215959, 66.3711...</td>\n",
       "      <td>[65.95946801773275, 65.94363521215959, 66.3711...</td>\n",
       "      <td>[60.76630778974034, 63.220392653578216, 64.075...</td>\n",
       "      <td>[65.95946801773275, 65.94363521215959, 66.3711...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, FlanT5-XL)</th>\n",
       "      <td>[64.56618112729575, 66.5769474350855, 66.37112...</td>\n",
       "      <td>[64.56618112729575, 66.5769474350855, 66.37112...</td>\n",
       "      <td>[64.56618112729575, 66.5769474350855, 66.37112...</td>\n",
       "      <td>[64.56618112729575, 66.5769474350855, 66.37112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, FlanT5-XL)</th>\n",
       "      <td>[63.23622545915136, 53.79987333755542, 64.9620...</td>\n",
       "      <td>[63.23622545915136, 53.79987333755542, 64.9620...</td>\n",
       "      <td>[63.23622545915136, 53.79987333755542, 64.9620...</td>\n",
       "      <td>[63.23622545915136, 53.79987333755542, 64.9620...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, FlanT5-XL)</th>\n",
       "      <td>[65.12032932235591, 66.8777707409753, 67.84357...</td>\n",
       "      <td>[60.148828372387584, 63.91703609879671, 63.790...</td>\n",
       "      <td>[65.12032932235591, 66.8777707409753, 67.84357...</td>\n",
       "      <td>[65.12032932235591, 66.8777707409753, 67.84357...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, FlanT5-XL)</th>\n",
       "      <td>[65.83280557314755, 64.66117796073463, 65.5636...</td>\n",
       "      <td>[65.50031665611147, 64.48701709943002, 63.5845...</td>\n",
       "      <td>[62.127929069031026, 63.42621912602914, 65.373...</td>\n",
       "      <td>[62.365421152628244, 62.55541481950602, 67.463...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (attn, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (ffn, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (self-attn, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (cross-attn, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former and LLM LoRA (all, Vicuna-7B)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (self-attn QVO, FlanT5-XL)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QVO, FlanT5-XL)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q-Former (cross-attn QV, FlanT5-XL)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    r = 1  \\\n",
       "LLM LoRA (ffn, FlanT5-XL)               [66.03863204559848, 68.84103863204561, 68.5877...   \n",
       "LLM LoRA (attn, FlanT5-XL)              [63.01456618112729, 67.32108929702343, 68.0018...   \n",
       "LLM LoRA (all, FlanT5-XL)               [66.95693476884104, 66.35528815706144, 70.4243...   \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [65.95946801773275, 65.94363521215959, 66.3711...   \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [64.56618112729575, 66.5769474350855, 66.37112...   \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [63.23622545915136, 53.79987333755542, 64.9620...   \n",
       "Q-Former LoRA (all, FlanT5-XL)          [65.12032932235591, 66.8777707409753, 67.84357...   \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [65.83280557314755, 64.66117796073463, 65.5636...   \n",
       "LLM LoRA (ffn, Vicuna-7B)                                                             0.0   \n",
       "LLM LoRA (attn, Vicuna-7B)                                                            0.0   \n",
       "LLM LoRA (all, Vicuna-7B)                                                             0.0   \n",
       "Q-Former LoRA (ffn, Vicuna-7B)                                                        0.0   \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)                                                  0.0   \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)                                                 0.0   \n",
       "Q-Former LoRA (all, Vicuna-7B)                                                        0.0   \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)                                                0.0   \n",
       "Q-Former (self-attn QVO, FlanT5-XL)                                                   0.0   \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)                                                  0.0   \n",
       "Q-Former (cross-attn QV, FlanT5-XL)                                                   0.0   \n",
       "\n",
       "                                                                                    r = 2  \\\n",
       "LLM LoRA (ffn, FlanT5-XL)               [67.19442685243826, 68.1760607979734, 69.49018...   \n",
       "LLM LoRA (attn, FlanT5-XL)              [65.83280557314755, 67.59024699176695, 69.5851...   \n",
       "LLM LoRA (all, FlanT5-XL)               [67.79607346421786, 68.99936668777707, 68.8568...   \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [65.95946801773275, 65.94363521215959, 66.3711...   \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [64.56618112729575, 66.5769474350855, 66.37112...   \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [63.23622545915136, 53.79987333755542, 64.9620...   \n",
       "Q-Former LoRA (all, FlanT5-XL)          [60.148828372387584, 63.91703609879671, 63.790...   \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [65.50031665611147, 64.48701709943002, 63.5845...   \n",
       "LLM LoRA (ffn, Vicuna-7B)                                                             0.0   \n",
       "LLM LoRA (attn, Vicuna-7B)                                                            0.0   \n",
       "LLM LoRA (all, Vicuna-7B)                                                             0.0   \n",
       "Q-Former LoRA (ffn, Vicuna-7B)                                                        0.0   \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)                                                  0.0   \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)                                                 0.0   \n",
       "Q-Former LoRA (all, Vicuna-7B)                                                        0.0   \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)                                                0.0   \n",
       "Q-Former (self-attn QVO, FlanT5-XL)                                                   0.0   \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)                                                  0.0   \n",
       "Q-Former (cross-attn QV, FlanT5-XL)                                                   0.0   \n",
       "\n",
       "                                                                                    r = 4  \\\n",
       "LLM LoRA (ffn, FlanT5-XL)               [66.07029765674477, 66.2444585180494, 67.47941...   \n",
       "LLM LoRA (attn, FlanT5-XL)              [64.70867637745408, 67.25775807473084, 68.2552...   \n",
       "LLM LoRA (all, FlanT5-XL)               [66.56111462951235, 68.09689677010766, 71.1526...   \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [60.76630778974034, 63.220392653578216, 64.075...   \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [64.56618112729575, 66.5769474350855, 66.37112...   \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [63.23622545915136, 53.79987333755542, 64.9620...   \n",
       "Q-Former LoRA (all, FlanT5-XL)          [65.12032932235591, 66.8777707409753, 67.84357...   \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [62.127929069031026, 63.42621912602914, 65.373...   \n",
       "LLM LoRA (ffn, Vicuna-7B)                                                             0.0   \n",
       "LLM LoRA (attn, Vicuna-7B)                                                            0.0   \n",
       "LLM LoRA (all, Vicuna-7B)                                                             0.0   \n",
       "Q-Former LoRA (ffn, Vicuna-7B)                                                        0.0   \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)                                                  0.0   \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)                                                 0.0   \n",
       "Q-Former LoRA (all, Vicuna-7B)                                                        0.0   \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)                                                0.0   \n",
       "Q-Former (self-attn QVO, FlanT5-XL)                                                   0.0   \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)                                                  0.0   \n",
       "Q-Former (cross-attn QV, FlanT5-XL)                                                   0.0   \n",
       "\n",
       "                                                                                    r = 8  \n",
       "LLM LoRA (ffn, FlanT5-XL)               [66.2444585180494, 67.30525649145028, 70.55098...  \n",
       "LLM LoRA (attn, FlanT5-XL)              [65.84863837872071, 67.2102596580114, 68.90436...  \n",
       "LLM LoRA (all, FlanT5-XL)               [65.880303989867, 67.97023432552248, 68.936035...  \n",
       "Q-Former LoRA (ffn, FlanT5-XL)          [65.95946801773275, 65.94363521215959, 66.3711...  \n",
       "Q-Former LoRA (self-attn, FlanT5-XL)    [64.56618112729575, 66.5769474350855, 66.37112...  \n",
       "Q-Former LoRA (cross-attn, FlanT5-XL)   [63.23622545915136, 53.79987333755542, 64.9620...  \n",
       "Q-Former LoRA (all, FlanT5-XL)          [65.12032932235591, 66.8777707409753, 67.84357...  \n",
       "Q-Former and LLM LoRA (all, FlanT5-XL)  [62.365421152628244, 62.55541481950602, 67.463...  \n",
       "LLM LoRA (ffn, Vicuna-7B)                                                             0.0  \n",
       "LLM LoRA (attn, Vicuna-7B)                                                            0.0  \n",
       "LLM LoRA (all, Vicuna-7B)                                                             0.0  \n",
       "Q-Former LoRA (ffn, Vicuna-7B)                                                        0.0  \n",
       "Q-Former LoRA (self-attn, Vicuna-7B)                                                  0.0  \n",
       "Q-Former LoRA (cross-attn, Vicuna-7B)                                                 0.0  \n",
       "Q-Former LoRA (all, Vicuna-7B)                                                        0.0  \n",
       "Q-Former and LLM LoRA (all, Vicuna-7B)                                                0.0  \n",
       "Q-Former (self-attn QVO, FlanT5-XL)                                                   0.0  \n",
       "Q-Former (cross-attn QVO, FlanT5-XL)                                                  0.0  \n",
       "Q-Former (cross-attn QV, FlanT5-XL)                                                   0.0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_epoch_df.to_csv(f'{dataset}/val_acc_per_epoch.csv', index=True)\n",
    "val_acc_per_epoch_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
